# Inference for regression {#inference-reg}

::: {.underconstruction}
This chapter is currently under construction, however the content to be presented in this chapter is covered in the R tutorials. We encourage you to review the content there in the meantime. Click [here](https://openintrostat.github.io/ims-tutorials/08-inference-for-regression/) to take a look! 
:::

::: {.chapterintro}
Chapter intro
:::


## Inference for linear regression


In this chapter, we bring together the inferential ideas (see Chapter \@ref(intro-stat-inference)) used to make claims about a population from information in a sample and the modeling ideas seen in Chapters \@ref(intro-linear-models) and \@ref(multi-logistic-models). 
In particular, we will use the least squares regression line to test whether or not there is a relationship between two continuous variables. 
Additionally, we will build confidence intervals which quantify the slope of the linear regression line.

### Randomization test for $H_0: \beta_1= 0$ {#randslope}

By repeatedly permuting the response variable any pattern in the linear model that is observed is due only to random chance (and not an underlying relationship).  The randomization test compares the slopes calculated from the permuted response variable with the observed slope.  If the observed slope is inconsistent with the slopes from permuting, we can conclude that there is some underlying relationship (and that the slope is not merely due to random chance).



#### Observed data {-}

#### Variability of the statistic {-}

As you can see, sometimes the slope of the permuted data is positive, sometimes it is negative.  Because the randomization happens under the condition of no underlying relationship (because the response variable is completely mixed with the explanatory variable), we expect to see the center of the randomized slope distribution to be zero.


#### Observed statistic vs. null statistics {-}


### Bootstrap confidence interval for $\beta_1$ {#bootbeta1}


As we have seen in previous chapters, we can use bootstrapping to estimate the sampling distribution of the statistic of interest (here, the slope) without any assumption of no relationship (which was the condition in the randomization test).  Because interest is now in creating a CI, there is no null hypothesis, so there won't be any reason to permute either of the variables.


#### Observed data {-}

#### Variability of the statistic {-}



### Mathematical model {#mathslope}

When certain technical assumptions apply, it is convenient to use mathematical approximations to test and estimate the slope parameter. 
The approximations will build on the t-distribution which were described in Chapter \@ref(inference-num). The mathematical model is often correct and is usually easy to implement computationally.
The validity of the technical conditions will be considered in detail in Section \@ref(tech-cond-linmod). 

#### Observed data {-}

#### Variability of the statistic {-}

#### Observed statistic vs. null statistics {-}

### Exercises 

::: {.underconstruction}
Exercises for this section are under construction.
:::

## Checking model assumptions {#tech-cond-linmod}

In the previous sections, we used randomization and bootstrapping to perform inference when the mathematical model was not valid due to violations of the technical conditions.  In this section, we'll provide details for when the mathematical model is appropriate.

#### What are the technical conditions? {-}

$$Y = \beta_0 + \beta_1 \cdot X + \epsilon$$

$$\epsilon \sim N(0, \sigma_\epsilon)$$ 

* L: __linear__ model 
* I: __independent__ observations 
* N: points are __normally__ distributed around the line 
* E: __equal__ variability around the line for all values of the explanatory variable 

### Exercises 

## Inference for multiple regression


Discussion will focus on interpreting coefficients (and their signs) in relationship to other variables.  We will note that the significance of a variable may change as other variables are included or removed from the model.  We will briefly mention both p-values and cross validation as methods for model building, but we will not use strict rules or algorithms for selecting variables based primarily on p-values.

For assessing model fit, as is necessary, we will create residual plots of residual vs. fitted (instead of residual vs. X as we saw previously).  

### Exercises 

::: {.underconstruction}
Exercises for this section are under construction.
:::

## Inference for logistic regression

As with multiple linear regression, the inference aspect for logistic regression will focus on interpretation of coefficients and relationships between explanatory variables.  Both p-values and cross validation will be used for model fitting.  

### Exercises 

::: {.underconstruction}
Exercises for this section are under construction.
:::

## Chapter review {#chp8-review}

### Terms

We introduced the following terms in the chapter. 
If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. 
However you should be able to easily spot them as **bolded text**.

```{r eval=FALSE}
make_terms_table(terms_chp_8)
```

### Chapter exercises

::: {.underconstruction}
Exercises for this section are under construction.
:::

::: {.sectionexercise}

```{r intro, child="08-exercises/08-05-chapter-review.Rmd", eval=FALSE}
```

:::

### Interactive R tutorials

Navigate the concepts you've learned in this chapter in R using the following self-paced tutorials. 
All you need is your browser to get started!

::: {.alltutorials}
[Tutorial 8: Inference for regression](https://openintrostat.github.io/ims-tutorials/08-inference-for-regression/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 1: Inference in regression](https://openintro.shinyapps.io/ims-08-inference-for-regression-01/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 2: Randomization test for slope](https://openintro.shinyapps.io/ims-08-inference-for-regression-02/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 3: t-test for slope](https://openintro.shinyapps.io/ims-08-inference-for-regression-03/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 4: Checking technical conditions for slope inference](https://openintro.shinyapps.io/ims-08-inference-for-regression-04/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 5: Inference beyond the simple linear regression model](https://openintro.shinyapps.io/ims-08-inference-for-regression-05/)
:::

You can also access the full list of tutorials supporting this book [here](https://openintrostat.github.io/ims-tutorials/).

### R labs

Further apply the concepts you've learned in this chapter in R with computational labs that walk you through a data analysis case study.

::: {.singlelab}
[Multiple linear regression - Grading the professor](http://openintrostat.github.io/oilabs-tidy/09_multiple_regression/multiple_regression.html)
:::

::: {.alllabs}
[Full list of labs supporting OpenIntro::Introduction to Modern Statistics](http://openintrostat.github.io/oilabs-tidy/)
:::
