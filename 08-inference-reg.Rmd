# Inference for regression {#inference-reg}

::: {.underconstruction}
This chapter is currently under construction, however the content to be presented in this chapter is covered in the R tutorials. We encourage you to review the content there in the meantime. Click [here](https://openintrostat.github.io/ims-tutorials/08-inference-for-regression/) to take a look! 
:::

::: {.chapterintro}
We now bring together ideas of inferential analyses from Chapter \@ref(intro-stat-inference) with the descriptive models seen in Chapters \@ref(intro-linear-models) and \@ref(multi-logistic-models).  The setting is now focused on predicting a numeric response variable (for linear models) or a binary response variable (for logistic models), we continue to ask questions about the variability of the model from sample to sample.  The sampling variability will inform the conclusions about the population that can be drawn.

Many of the inferential ideas are remarkably similar to those covered in previous chapters.  The technical conditions for linear models are typically assessed graphically, although independence of observations continues to be of utmost importance.  

We encourage the reader to think broadly about the models at hand without putting too much dependence on the exact p-values that are reported from the statistical software.  Inference on models with multiple explanatory variables can suffer from data snooping which result in false positive claims.  We provide some guidance and hope the reader will further their statistical learning after working through the material in this text.
:::


## Inference for linear regression {#inferenceForLinearRegression}


In this chapter, we bring together the inferential ideas (see Chapter \@ref(intro-stat-inference)) used to make claims about a population from information in a sample and the modeling ideas seen in Chapters \@ref(intro-linear-models) and \@ref(multi-logistic-models). 
In particular, we will use the least squares regression line to test whether or not there is a relationship between two continuous variables. 
Additionally, we will build confidence intervals which quantify the slope of the linear regression line.



#### Observed data {-}

Consider the data on Global Crop Yields compiled by [Our World in Data](https://ourworldindata.org/crop-yields) and presented as part of the [TidyTuesday](https://github.com/rfordatascience/tidytuesday/trunk/data/2020/2020-09-01) series.  The scientific research interest at hand will be in determining the linear relationship between wheat yield (for a country-year) and other crop yields.  The dataset is quite rich and deserves exploring, but for this example, we will focus only on the annual crop yield in the United States.  

```{r echo = FALSE, fig.cap = "Yield (in tonnes per hectare) for six different crops in the US.  The color of the dot indicates the year."}
key_crop_yields <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv') %>%
  rename(wheat = `Wheat (tonnes per hectare)`,
         rice = `Rice (tonnes per hectare)`,
         maize = `Maize (tonnes per hectare)`,
         soybeans = `Soybeans (tonnes per hectare)`,
         potatoes = `Potatoes (tonnes per hectare)`,
         beans = `Beans (tonnes per hectare)`,
         peas = `Peas (tonnes per hectare)`,
         cassava = `Cassava (tonnes per hectare)`,
         barley = `Barley (tonnes per hectare)`,
         cocoa = `Cocoa beans (tonnes per hectare)`,
         bananas = `Bananas (tonnes per hectare)`)
  
crops_USA <- key_crop_yields %>%
  filter(Code == "USA") %>%
  select(-cassava, -cocoa, -potatoes, -bananas)

crops_USA_long <- crops_USA %>%
  pivot_longer(rice:barley,
    names_to = "crop", values_to = "yield")

ggplot(crops_USA_long)   +
    geom_point(aes(x = yield, y = wheat, color = Year)) +
  facet_wrap(~crop)
```

#### Variability of the statistic {-}

As with any numerical characteristic which describes a subset of the population, the estimated slope of a sample will vary from sample to sample.  Consider the linear model which describes wheat production (in tonnes per hectare) based on maize production (in tonnes per hectare).  A random sample of 20 years shows a different least square regression line depending on which observations are selected.

```{r echo = FALSE}
set.seed(4747)
crops2 <- crops_USA %>% 
  sample_n(size=20) 
crops3 <- crops_USA %>%
  sample_n(size=20)
crops_many <- crops_USA %>%
  rep_sample_n(size = 20, replace = FALSE, reps = 50)
```


```{r echo = FALSE, fig.cap = "Wheat yield as a linear model of maize yield, in tonnes per hectare."}
ggplot(crops_USA) +
  geom_point(aes(x = maize, y = wheat)) +
  geom_smooth(aes(x = maize, y = wheat), method = "lm", se = FALSE,color=COL[1,1]) 
```


A subset of size 20 items shows a similar positive trend between maize and wheat, despite having fewer observations on the plot.


```{r echo = FALSE, fig.cap = "A random sample of 20 years from the original dataset. A linear trend between `maize` and `wheat` continues to be observed."}
ggplot(crops2) +
  geom_point(aes(x = maize, y = wheat)) +
  geom_smooth(aes(x = maize, y = wheat), method = "lm", se = FALSE, color = COL[1,1]) 
```


A second sample of size 20 also shows a positive trend!

```{r echo = FALSE, fig.cap = "A different random sample of 20 years from the original dataset. Again, a linear trend between `maize` and `wheat` is observed."}

ggplot(crops3) +
  geom_point(aes(x = maize, y = wheat)) +
  geom_smooth(aes(x = maize, y = wheat), method = "lm", se = FALSE,color=COL[4,1]) 
```


But the line is slightly different!

```{r echo = FALSE, fig.cap = "The linear models from the two different random samples are quite similar, but they are not the same line."}
ggplot(crops2, aes(x = maize, y = wheat)) + geom_point(color=COL[1,1]) + 
  geom_smooth(method="lm", se=FALSE, color=COL[1,1]) + 
  geom_point(data=crops3, color=COL[4,1]) +
  geom_smooth(data=crops3, method="lm", se=FALSE, color=COL[4,1])
```


That is, there is variability in the regression line from sample to sample.  The concept of the sampling variability is something you've seen before, but in this lesson, you will focus on the variability of the line often measured through the variability of a single statistic:  the slope of the line.

```{r echo = FALSE, fig.cap = "If repeated samples of size 20 are taken from the original data, each linear model will be slightly different."}
ggplot(crops_many, aes(x=maize, y=wheat, group=replicate)) + 
  geom_point() + 
  ggtitle("maize vs. wheat for annual crop yield in the US (n=20)") + 
  geom_smooth(method="lm", se=FALSE, color=COL[1,1])
```  


The distribution of slopes (for samples of size $n=20$) can be seen in a histogram.

```{r echo = FALSE}
crops_many_lm <- crops_many %>% 
  group_by(replicate) %>% 
  do(tidy(lm(wheat ~ maize, data=.))) %>%
  filter(term=="maize")

ggplot(crops_many_lm, aes(x=estimate)) + 
  geom_histogram() +
  xlab("slope estimates for different samples of size n=20")
```

We have seen variability in samples throughout this text, so it should not come as a surprise that different samples will produce different linear models.
However, it is nice to visually consider the linear models produced by different slopes.
Additionally, as with measuring the variability of previous statistics (e.g., $\overline{X}_1 - \overline{X}_2$ or $\hat{p}_1 - \hat{p}-2$), the histogram of the sample statistics can provide information related to inferential considerations.

In the following sections, the distribution (i.e., histogram) of $b_1$ (the estimated slope coefficient) will be constructed in the same three ways that, by now, may be familiar to you.
First (in Section \@ref(randslope)), the distribution of $b_1$ when $\beta_1 = 0$ is constructed by randomizing (permuting) the response variable.
Next (in Section \@ref(bootbeta1)), we can bootstrap the data by taking random samples of size n from the original dataset.
And last (in Section \@ref(mathslope)), we use mathematical tools to describe the variability using the $t$-distribution that was first encountered in Section \@ref(one-mean-math).  

### Randomization test for $H_0: \beta_1= 0$ {#randslope}

As you have seen previously, statistical inference typically relies on setting a null hypothesis which is hoped to be subsequently rejected.  In the linear model setting, we might hope to have a linear relationship between `maize` and `wheat` in settings where `maize` production is known and `wheat` production needs to be predicted.  

The relevant hypotheses for the linear model setting can be written in terms of the population slope parameter.  Here the population refers to a larger set of years where `maize` and `wheat` are both grown in the US.

* $H_0: \beta_1= 0$, there is no linear relationship between `wheat` and `maize`.  
* $H_A: \beta_1 \ne 0$, there is some linear relationship between `wheat` and `maize`.

Recall that for the randomization test, we permute one variables to eliminate any existing relationship between the variables.  That is, we set the null hypothesis to be true, and we measure the natural variability in the data due to sampling but **not** due to variables being correlated.  Figure \@ref(fig:permwheatScatter) shows the observed data and a scatterplot of one permutation of the `wheat` variable.  The careful observer can see that each of the observed the values for `wheat` (and for `maize`) exist in both the original data plot as well as the permuted `wheat` plot, but the given `wheat` and `maize` yields are no longer matched for a given year.  That is, each `wheat` yield is randomly assigned to a new `maize` yield.

```{r permwheatScatter, fig.cap = "Original (left) and permuted (right) data.  The permutation removes the linear relationship between `wheat` and `maize`.  Repeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true).", fig.show = 'hold', out.width='47%', fig.ncol = 2}
set.seed(4747)
ggplot(crops_USA) +
  geom_point(aes(x = maize, y = wheat)) +
  ggtitle("Original Data")

ggplot(crops_USA) +
  geom_point(aes(x = maize, y = sample(wheat))) +
  ylab("permuted wheat") +
  ggtitle("Permuted Data")
```


By repeatedly permuting the response variable any pattern in the linear model that is observed is due only to random chance (and not an underlying relationship).  The randomization test compares the slopes calculated from the permuted response variable with the observed slope.  If the observed slope is inconsistent with the slopes from permuting, we can conclude that there is some underlying relationship (and that the slope is not merely due to random chance).

#### Observed data {-}

We will continue to use the crop data to investigate the linear relationship between `wheat` and `maize`.  Note that the least squares model (see Chapter \@ref(intro-linear-models) ) describing the relationship is given in Table \@ref(tab:lsCrops).  The columns in Table \@ref(tab:lsCrops) are further described in Section \@ref(mathslope).

```{r lsCrops}
lm(wheat ~ maize, data = crops_USA) %>% 
  tidy() %>%
 kable(caption = "The least squares estimates of the intercept and slope are given in the `estimate` column.  The observed slope is 0.195.") %>%
 kable_styling() 
```

#### Variability of the statistic {-}

After permuting the data, the least squares estimate of the line can be computed.  Repeated permutations and slope calculations describe the variability in the line (i.e., in the slope) due only to the natural variability and not due to a relationship between `wheat` and `maize`.  Figure \@ref(fig:permwheatlm) shows two different permutations of `wheat` and the resulting linear models.

```{r permwheatlm, fig.cap = "Two different permutations of the wheat variable with slightly different least squares regression lines.", fig.show = 'hold', fig.ncol = 2, out.width='47%'}
set.seed(47)
ggplot(crops_USA, aes(x = maize, y = sample(wheat))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = COL[1,1]) +
  ylab("permuted wheat") +
  ggtitle("First Permutation of Wheat")

ggplot(crops_USA, aes(x = maize, y = sample(wheat))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = COL[4,1]) +
  ylab("permuted wheat") +
  ggtitle("Second Permutation of Wheat")
```


As you can see, sometimes the slope of the permuted data is positive, sometimes it is negative.  Because the randomization happens under the condition of no underlying relationship (because the response variable is completely mixed with the explanatory variable), we expect to see the center of the randomized slope distribution to be zero.

#### Observed statistic vs. null statistics {-}

```{r nulldistCrop, echo=FALSE, fig.cap = "Histogram of slopes given different permutations of the wheat variable.  The vertical red line is at the observed value of the slope, 0.195."}
perm_slope <- crops_USA %>%
   specify(wheat ~ maize) %>%
   hypothesize(null = "independence") %>%
   generate(reps = 1000, type = "permute") %>%
   calculate(stat = "slope") 
 
obs_slope <- crops_USA %>%
   specify(wheat ~ maize) %>%
   calculate(stat = "slope") %>% pull()
  
ggplot(data=perm_slope, aes(x=stat)) + 
   geom_histogram() +
  geom_vline(xintercept = obs_slope, col = COL[4,1])
```

As we can see from Figure \@ref(fig:nulldistCrop), a slope estimate as extreme as the observed slope estimate (the red line) never happened in many repeated permutations of the `wheat` variable.
That is, if indeed there were no linear relationship between `wheat` and `maize`, the natural variability of the slopes would produce estimates between approximately -0.1 and +0.1.
We reject the null hypothesis.
Therefore, we believe that the slope observed on the original data is not just due to natural variability and indeed, there is a linear relationship between `wheat` and `maize` crop yield in the US.



### Bootstrap confidence interval for $\beta_1$ {#bootbeta1}


As we have seen in previous chapters, we can use bootstrapping to estimate the sampling distribution of the statistic of interest (here, the slope) without the null assumption of no relationship (which was the condition in the randomization test).  Because interest is now in creating a CI, there is no null hypothesis, so there won't be any reason to permute either of the variables.


#### Observed data {-}

Returning to the crop data, we may want to consider the relationship between `peas` and `wheat`.  Are `peas` a good predictor of `wheat`?  And if so, what is their relationship?  That is, what is the slope that models `average wheat yield` as a function of `peas`?

```{r echo = FALSE}
set.seed(4747)
crops4 <- crops_USA %>% 
  sample_n(size=58, replace = TRUE) 
crops5 <- crops_USA %>%
  sample_n(size=58, replace = TRUE)
crops_many_BS <- crops_USA %>%
  rep_sample_n(size = 58, replace = TRUE, reps = 50)
```


```{r echo = FALSE, fig.cap = "Original data: wheat yield as a linear model of peas yield, in tonnes per hectare."}
ggplot(crops_USA) +
  geom_point(aes(x = peas, y = wheat)) +
  geom_smooth(aes(x = peas, y = wheat), method = "lm", se = FALSE,color=COL[1,1]) 
```




#### Variability of the statistic {-}

Because we are not focused on a null distribution, we sample with replacement $n=58$ observations from the original dataset.  Recall that with bootstrapping we always resample the same number of observations as we start with in order to mimic the process of taking a sample from the population.  When sampling in the linear model case, consider each observation to be a single dot.  If the dot is resampled, both the `wheat` and the `peas` measurement are observed.  The measurements are linked to the dot (i.e., to the year in which the measurements were taken).

```{r crop2BS, echo = FALSE, fig.cap = "Two bootstrap samples of the crop data.  Note that it is difficult to differentiate the two plots, as (within a single bootstrap sample) the observations which have been resampled twice are plotted as points on top of one another.", fig.show = 'hold', out.width = '47%', fig_ncol = 2}

ggplot(crops4) +
  geom_point(aes(x = maize, y = wheat)) +
  geom_smooth(aes(x = maize, y = wheat), method = "lm", se = FALSE,col=COL[4,1]) +
  ggtitle("First bootstrap sample from the crop data.")


ggplot(crops5) +
  geom_point(aes(x = maize, y = wheat)) +
  geom_smooth(aes(x = maize, y = wheat), method = "lm", se = FALSE,col=COL[1,1]) +
  ggtitle("Second bootstrap sample from the crop data.")
```


```{r echo = FALSE, fig.cap = "Repeated bootstrap resamples of size 58 are taken from the original data.  Each linear model is slightly different."}
ggplot(crops_many_BS, aes(x=peas, y=wheat, group=replicate)) + 
  geom_point() + 
  geom_smooth(method="lm", se=FALSE, color=COL[1,1])
```  


The distribution of slopes (for samples of size $n=20$) can be seen in a histogram.

```{r echo = FALSE}
crops_many_lm_BS <- crops_many_BS %>% 
  group_by(replicate) %>% 
  do(tidy(lm(wheat ~ peas, data=.))) %>%
  filter(term=="peas")

ggplot(crops_many_lm, aes(x=estimate)) + 
  geom_histogram() +
  xlab("slope estimates for different bootstrap resamples")
```


### Mathematical model {#mathslope}

When certain technical assumptions apply, it is convenient to use mathematical approximations to test and estimate the slope parameter. 
The approximations will build on the t-distribution which were described in Chapter \@ref(inference-num). 
The mathematical model is often correct and is usually easy to implement computationally.
The validity of the technical conditions will be considered in detail in Section \@ref(tech-cond-linmod). 


In this section, we discuss uncertainty in the estimates of the slope
and y-intercept for a regression line. Just as we identified standard
errors for point estimates in previous chapters, we first discuss
standard errors for these new estimates.

#### Midterm elections and unemployment {-}

##### Observed data {-}

\index{data!midterm elections|(}

Elections for members of the United States House of Representatives
occur every two years, coinciding every four years with the U.S.
Presidential election. The set of House elections occurring during the
middle of a Presidential term are called midterm elections. In America's two-party
system, one political theory suggests the higher the unemployment rate,
the worse the President's party will do in the midterm elections.

\indexthis{midterm elections}{midterm election}

To assess the validity of this claim, we can compile historical data and
look for a connection. We consider every midterm election from 1898 to
2018, with the exception of those elections during the Great Depression.

Figure \@ref(fig:unemploymentAndChangeInHouse) shows these data and the
least-squares regression line: 
$$\begin{aligned}
&\text{% change in House seats for President's party}  \\
&\qquad\qquad= -7.36 - 0.89 \times \text{(unemployment rate)}\end{aligned}$$
We consider the percent change in the number of seats of the President's
party (e.g. percent change in the number of seats for Republicans in
2018) against the unemployment rate.

Examining the data, there are no clear deviations from linearity, the
constant variance condition, or substantial outliers. While the data are
collected sequentially, a separate analysis was used to check for any
apparent correlation between successive observations; no such
correlation was found.

```{r unemploymentAndChangeInHouse, fig.cap="The percent change in House seats for the President's party in each election from 1898 to 2010 plotted against the unemployment rate. The two points for the Great Depression have been removed, and a least squares regression line has been fit to the data.", warning=FALSE, fig.width=15}

d <- midterms_house
th <- !d$year %in% c(1935, 1939)
plot(d$unemp[th], d$house_change[th],
     # col = COL[ifelse(d$party[th] == "Republican", 4, 1)],
     pch = 19,
     xlim = c(3, 12),
     ylim = c(-30, 13),
     axes = FALSE,
     type = 'n',
     xlab = '',
     ylab = paste0("Percent Change in Seats of\n",
                  "President's Party in House of Rep."))
mtext('Percent Unemployment', 1, 2)
abline(h = seq(-100, 100, 10), col = COL[7, 3], lwd = 2)
abline(h = seq(-105, 100, 10), col = COL[7, 3], lwd = 0.7)
abline(v = seq(-100, 100, 4), col = COL[7, 3], lwd = 2)
abline(v = seq(-102, 100, 4), col = COL[7, 3], lwd = 0.7)
repub <- (d$party[th] == "Republican")
points(d$unemp[th], d$house_change[th],
       col = COL[ifelse(repub, 4, 1)],
       pch = ifelse(repub, 17, 19))
AxisInPercent(1, at = seq(0, 20, 4))
AxisInPercent(2, at = seq(-100, 100, 10))
box()
cases <- c(1, 22, 25, 27, 29, 31)
for (i in 1:length(cases)) {
  potus  <- as.character(d$potus[cases[i]])
  potus  <- tail(strsplit(potus, " ")[[1]], 1)
  year   <- d$year[cases[i]]-1
  potus  <- paste0(potus, "\n", year)
  unem   <- d$unemp[cases[i]]
  change <- d$house_change[cases[i]]
  text(unem, change, potus, pos = 3, cex = 0.6)
}

#summary(lm(house_change ~ unemp, d))

g <- lm(house_change ~ unemp, d[th,])
#summary(g)
abline(g, col = COL[5])
legend('topright',
       bg = "#FFFFFF",
       pch = c(19, 17),
       col = COL[c(1, 4)],
       legend = c("Democrat", "Republican"))


```


::: {.guidedpractice}
The data for the Great Depression (1934 and 1938) were removed because
the unemployment rate was 21% and 18%, respectively. Do you agree that
they should be removed for this investigation? Why or why not?^[We will provide two considerations.
  Each of these points would have very high leverage on any
  least-squares regression line, and years with such high
  unemployment may not help us understand what would happen
  in other years where the unemployment is only modestly high.
  On the other hand, these are exceptional cases, and we would
  be discarding important information if we exclude them from
  a final analysis.]
:::
  
There is a negative slope in the line shown in Figure \@ref(fig:unemploymentAndChangeInHouse). However, this slope (and the
y-intercept) are only estimates of the parameter values. We might
wonder, is this convincing evidence that the "true" linear model has a
negative slope? That is, do the data provide strong evidence that the
political theory is accurate, where the unemployment rate is a useful
predictor of the midterm election? We can frame this investigation into
a statistical hypothesis test:

* $H_0$: $\beta_1 = 0$. The true linear model has slope zero.  
* $H_A$: $\beta_1 \neq 0$. The true linear model has a slope different than
    zero. The unemployment is predictive of whether the President's
    party wins or loses seats in the House of Representatives.

We would reject $H_0$ in favor of $H_A$ if the data provide strong
evidence that the true slope parameter is different than zero. To assess
the hypotheses, we identify a standard error for the estimate, compute
an appropriate test statistic, and identify the p-value.

#### Understanding regression output from software {-}


##### Variability of the statistic {-}

Just like other point estimates we have seen before, we can compute a
standard error and test statistic for $b_1$. We will generally label the
test statistic using a $T$, since it follows the $t$-distribution.

We will rely on statistical software to compute the standard error and
leave the explanation of how this standard error is determined to a
second or third statistics course.
Table \@ref(tab:midtermUnempRegTable) shows software output for the least
squares regression line in Figure \@ref(fig:unemploymentAndChangeInHouse). The row labeled `unemp` includes the point estimate and other hypothesis test information for
the slope, which is the coefficient of the unemployment variable.



```{r midtermUnempRegTable}
d <- midterms_house
th <- !d$year %in% c(1935, 1939)
lm(house_change ~ unemp, d[th,]) %>% 
  tidy() %>%
 kable(caption = "Output from statistical software for the regression
    line modeling the midterm election losses for the
    President's party as a response to unemployment.") %>%
 kable_styling() 
```


What do the first and second columns of Table \@ref(tab:midtermUnempRegTable)  represent? The entries in the first
column represent the least squares estimates, $b_0$ and $b_1$, and the
values in the second column correspond to the standard errors of each
estimate. Using the estimates, we could write the equation for the least
square regression line as $$\begin{aligned}
  \hat{y} = -7.3644 - 0.8897 x
  \end{aligned}$$ where $\hat{y}$ in this case represents the predicted
change in the number of seats for the president's party, and $x$
represents the unemployment rate.


We previously used a $t$-test statistic for hypothesis testing in the
context of numerical data. Regression is very similar. In the hypotheses
we consider, the null value for the slope is 0, so we can compute the
test statistic using the T (or Z) score formula: 
$$\begin{aligned}
T
  = \frac{\text{estimate} - \text{null value}}{\text{SE}}
  = \frac{-0.8897 - 0}{0.8350}
  = -1.07\end{aligned}$$ This corresponds to the third column of
Table \@ref(tab:midtermUnempRegTable) .


::: {.example}
Use Table \@ref(tab:midtermUnempRegTable) to determine the p-value for the
hypothesis test. 

---

The last column of the table gives the p-value for the
two-sided hypothesis test for the coefficient of the unemployment rate:
0.2961. That is, the data do not provide convincing evidence that a
higher unemployment rate has any correspondence with smaller or larger
losses for the President's party in the House of Representatives in
midterm elections.
:::




##### Observed statistic vs. null statistics {-}


::: {.important}
**Inference for regression**
We usually rely on statistical software to
identify point estimates, standard errors, test statistics, and p-values
in practice. However, be aware that software will not generally check
whether the method is appropriate, meaning we must still verify
conditions are met.
:::

::: {.example}
Examine Figure \@ref(fig:elmhurstScatterWLSROnly-CLTsection), which relates the Elmhurst College
aid and student family income. How sure are you that the slope is
statistically significantly different from zero? That is, do you think a
formal hypothesis test would reject the claim that the true slope of the
line should be zero?

---
While the relationship
between the variables is not perfect, there is an evident decreasing
trend in the data. This suggests the hypothesis test will reject the
null claim that the slope is zero.
:::



```{r rOutputForIncomeAidLSRLineInInferenceSection}
d <- elmhurst
d$gift_aid <- d$gift_aid * 1000
d$family_income <- d$family_income * 1000
lm(gift_aid ~ family_income, d) %>% 
  tidy() %>%
 kable(caption = "Summary of least squares fit for the Elmhurst College data, where we are predicting the gift aid by the university based on the family income of students.") %>%
 kable_styling() 
```


::: {.guidedpractice}
Table \@ref(tab:rOutputForIncomeAidLSRLineInInferenceSection) shows
statistical software output from fitting the least squares regression
line shown in Figure \@ref(fig:elmhurstScatterWLSROnly-CLTsection). Use the output to formally
evaluate the following hypotheses.

* $H_0$: The true coefficient for family income is zero.  
* $H_A$: The true coefficient for family income is not zero.^[We look in the second row corresponding
  to the family income variable.
  We see the point estimate of the slope of the line is -0.0431,
  the standard error of this estimate is 0.0108, and the $t$-test
  statistic is $T = -3.98$.
  The p-value corresponds exactly to the two-sided test we are
  interested in: 0.0002.
  The p-value is so small that we reject the null hypothesis
  and conclude that family income and financial aid at Elmhurst
  College for freshman entering in the year 2011 are negatively
  correlated and the true slope parameter is indeed less than 0,
  just as we believed in our analysis of Figure \@ref(fig:elmhurstScatterWLSROnly-CLTsection).]
:::  

#### Confidence interval for a coefficient {-}

##### Observed data {-}

Similar to how we can conduct a hypothesis test for a model coefficient
using regression output, we can also construct a confidence interval for
that coefficient.

::: {.example}
Compute the 95% confidence interval for the coefficient using the
regression output from Table \@ref(tab:rOutputForIncomeAidLSRLineInInferenceSection). 

---

The point estimate is -0.0431 and the standard error is $SE = 0.0108$. When
constructing a confidence interval for a model coefficient, we generally
use a $t$-distribution. The degrees of freedom for the distribution are
noted in the regression output, $df = 48$, allowing us to identify
$t_{48}^{\star} = 2.01$ for use in the confidence interval.

We can now construct the confidence interval in the usual way:
$$\begin{aligned}
  \text{point estimate} \pm t_{48}^{\star} \times SE
    \qquad\to\qquad -0.0431 \pm 2.01 \times 0.0108
    \qquad\to\qquad (-0.0648, -0.0214)
  \end{aligned}$$ We are 95% confident that with each dollar increase in
, the university's gift aid is predicted to decrease on average by
\$0.0214 to \$0.0648.
:::

##### Variability of the statistic {-}

::: {.important}
**Confidence intervals for coefficients** 

Confidence intervals for model
coefficients can be computed using the $t$-distribution:
$$\begin{aligned}
  b_i \ \pm\ t_{df}^{\star} \times SE_{b_{i}}
  \end{aligned}$$ where $t_{df}^{\star}$ is the appropriate $t$-value
corresponding to the confidence level with the model's degrees of
freedom.
:::

On the topic of intervals in this book, we've focused exclusively on
confidence intervals for model parameters. However, there are other
types of intervals that may be of interest, including prediction
intervals for a response value and also confidence intervals for a mean
response value in the context of regression. 

<!--These two interval types
are introduced in an online extra that you may download at

\oiRedirect{stat_extra_linear_regression_supp}
    {www.openintro.org/d?file=stat\_extra\_linear\_regression\_supp}

-----------
-->



### Exercises 

::: {.underconstruction}
Exercises for this section are under construction.
:::

## Checking model assumptions {#tech-cond-linmod}

In the previous sections, we used randomization and bootstrapping to perform inference when the mathematical model was not valid due to violations of the technical conditions.  In this section, we'll provide details for when the mathematical model is appropriate.

#### What are the technical conditions? {-}


When fitting a least squares line, we generally require

* **Linearity.**  The data should show a linear trend. If there is a nonlinear trend
    (e.g. first panel of Figure \@ref(fig:whatCanGoWrongWithLinearModel), an advanced regression
    method from another book or later course should be applied.

* **Independent observations.**  Be cautious about applying regression to data, which are sequential
    observations in time such as a stock price each day. Such data may
    have an underlying structure that should be considered in a model
    and analysis. An example of a data set where successive observations
    are not independent is shown in the fourth panel of
    Figure \@ref(fig:whatCanGoWrongWithLinearModel). There are also other
    instances where correlations within the data are important, which is
    further discussed in
    Chapter \@ref(multi-logistic-models).
    
* **Nearly normal residuals.**  Generally, the residuals must be nearly normal. When this condition
    is found to be unreasonable, it is usually because of outliers or
    concerns about influential points, which we'll talk about more in
    Section \2ref(outliers-in-regression). An example of a
    residual that would be a potentially concern is shown in
    Figure \@ref(fig:whatCanGoWrongWithLinearModel), where one observation is
    clearly much further from the regression line than the others.

* **Constant or equal variability.**  The variability of points around the least squares line remains
    roughly constant. An example of non-constant variability is shown in
    the third panel of
    Figure \@ref(fig:whatCanGoWrongWithLinearModel), which represents the
    most common pattern observed when this condition fails: the
    variability of $y$ is larger when $x$ is larger.


```{r whatCanGoWrongWithLinearModel, fig.cap="Four examples showing when the methods in this chapter are insufficient to apply to the data. First panel: linearity fails. Second panel: there are outliers, most especially one point that is very far away from the line. Third panel: the variability of the errors is related to the value of $x$. Fourth panel: a time series data set is shown, where successive observations are highly correlated.", warning=FALSE, fig.asp = 1.2}

source("08/figures/makeTubeAdv.R")
pch <- 20
cex <- 1.75
col <- COL[1, 3]

layout(matrix(1:8, 2),
       rep(1, 4),
       c(2, 1))

these <- simulated_scatter$group == 20
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
     axes = FALSE,
     pch = pch,
     cex = cex,
     col = "#00000000")
box()
makeTube(x, y,
         type = 'quad',
         addLine = FALSE,
         col = COL[7, 3])
points(x, y,
       pch = pch,
       cex = cex,
       col = COL[1, 2])
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
     xlab = "", ylab = "",
     axes = FALSE,
     pch = pch,
     cex = cex,
     col = COL[1, 2],
     ylim = yR)
abline(h = 0, lty = 2)
box()

these <- simulated_scatter$group == 21
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
     axes = FALSE,
     pch = pch,
     cex = cex,
     col = "#00000000")
box()
makeTube(x, y,
         addLine = FALSE,
         col = COL[7, 3])
points(x, y,
       pch = pch,
       cex = cex,
       col = COL[1,2])
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
     xlab = "", ylab = "",
     axes = FALSE,
     pch = pch,
     cex = cex,
     col = COL[1, 2],
     ylim = yR)
abline(h = 0, lty = 2)
box()

these <- simulated_scatter$group == 22
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
     axes = FALSE,
     pch = pch,
     cex = cex,
     col = "#00000000")
box()
makeTubeAdv(x, y,
            type = 'l',
            variance = 'l',
            bw = 0.03,
            Z = 1.7,
            col = COL[7, 3])
points(x, y,
       pch = pch,
       cex = cex,
       col = COL[1, 2])
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
     axes = FALSE,
     xlab = "", ylab = "",
     pch = pch,
     cex = cex,
     col = COL[1, 2],
     ylim = yR)
abline(h = 0, lty = 2)
box()

these <- simulated_scatter$group == 23
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
     axes = FALSE,
     pch = pch,
     cex = cex,
     col = "#00000000")
box()
makeTube(x, y,
         addLine = FALSE,
         col = COL[7, 3])
points(x, y,
       pch = pch,
       cex = cex,
       col = COL[1, 2])
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
     axes = FALSE,
     xlab = "", ylab = "",
     pch = pch,
     cex = cex,
     col = COL[1, 2],
     ylim = yR)
abline(h = 0, lty = 2)
box()

makeTubeAdv(x,y, col = COL[7,3])

```

::: {.guidedpractice}
Should we have concerns about applying least squares regression to the
Elmhurst data in Figure \@ref(fig:elmhurstScatterW2Lines)?^[The trend appears to be linear, the data fall around the line with no obvious outliers, the variance is roughly constant. These are also not time series observations. Least squares regression can be applied to these data.]
:::


The technical conditions are often remembered using the **LINE** mnemonic.
The linearity, normality, and equality of variance conditions usually can be assessed through residual plots, as seen in  Figure \@ref(fig:whatCanGoWrongWithLinearModel).
A careful consideration of the experimental design should be undertaken to confirm that the observed values are indeed independent.

* L: __linear__ model 
* I: __independent__ observations 
* N: points are __normally__ distributed around the line 
* E: __equal__ variability around the line for all values of the explanatory variable 



### Exercises 

## Inference for multiple regression


Discussion will focus on interpreting coefficients (and their signs) in relationship to other variables.  We will note that the significance of a variable may change as other variables are included or removed from the model.  We will briefly mention both p-values and cross validation as methods for model building, but we will not use strict rules or algorithms for selecting variables based primarily on p-values.

For assessing model fit, as is necessary, we will create residual plots of residual vs. fitted (instead of residual vs. X as we saw previously).  

### Exercises 

::: {.underconstruction}
Exercises for this section are under construction.
:::

## Inference for logistic regression

As with multiple linear regression, the inference aspect for logistic regression will focus on interpretation of coefficients and relationships between explanatory variables.  Both p-values and cross validation will be used for model fitting.  

### Exercises 

::: {.underconstruction}
Exercises for this section are under construction.
:::

## Chapter review {#chp8-review}

### Terms

We introduced the following terms in the chapter. 
If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. 
However you should be able to easily spot them as **bolded text**.

```{r eval=FALSE}
make_terms_table(terms_chp_8)
```

### Chapter exercises

::: {.underconstruction}
Exercises for this section are under construction.
:::

::: {.sectionexercise}

```{r intro, child="08-exercises/08-05-chapter-review.Rmd", eval=FALSE}
```

:::

### Interactive R tutorials

Navigate the concepts you've learned in this chapter in R using the following self-paced tutorials. 
All you need is your browser to get started!

::: {.alltutorials}
[Tutorial 8: Inference for regression](https://openintrostat.github.io/ims-tutorials/08-inference-for-regression/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 1: Inference in regression](https://openintro.shinyapps.io/ims-08-inference-for-regression-01/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 2: Randomization test for slope](https://openintro.shinyapps.io/ims-08-inference-for-regression-02/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 3: t-test for slope](https://openintro.shinyapps.io/ims-08-inference-for-regression-03/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 4: Checking technical conditions for slope inference](https://openintro.shinyapps.io/ims-08-inference-for-regression-04/)
:::

::: {.singletutorial}
[Tutorial 8 - Lesson 5: Inference beyond the simple linear regression model](https://openintro.shinyapps.io/ims-08-inference-for-regression-05/)
:::

You can also access the full list of tutorials supporting this book [here](https://openintrostat.github.io/ims-tutorials/).

### R labs

Further apply the concepts you've learned in this chapter in R with computational labs that walk you through a data analysis case study.

::: {.singlelab}
[Multiple linear regression - Grading the professor](http://openintrostat.github.io/oilabs-tidy/09_multiple_regression/multiple_regression.html)
:::

::: {.alllabs}
[Full list of labs supporting OpenIntro::Introduction to Modern Statistics](http://openintrostat.github.io/oilabs-tidy/)
:::
