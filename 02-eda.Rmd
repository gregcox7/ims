# Exploratory Data Analysis {#eda}

```{block2, chp1-intro, type="chapterintro", echo=TRUE}
This chapter focuses on the mechanics and construction of summary statistics and graphs.
We use statistical software for generating the summaries and graphs presented in this chapter and book.
However, since this might be your first exposure to these concepts, we take our time in this chapter to detail how to create them.
Mastery of the content presented in this chapter will be crucial for understanding the methods and techniques introduced in rest of the book.
```

## Examining numerical data {#numerical-data}

In this section we will explore techniques for summarizing numerical variables.
For example, consider the `loan_amount` variable from the `loan50` data set, which represents the loan size for all 50 loans in the data set.
This variable is numerical since we can sensibly discuss the numerical difference of the size of two loans.
On the other hand, area codes and zip codes are not numerical, but rather they are categorical variables.

Throughout this section and the next, we will apply these methods using the `loan50` and `county` data sets, which were introduced in Section \@ref(data-basics).
If you'd like to review the variables from either data set, see Tables \@ref(tab:loan50Variables) and \@ref(tab:countyVariables).

```{block2, type="data", echo=TRUE}
The [`county`](http://openintrostat.github.io/usdata/reference/county.html) data can be found in the [usdata](http://openintrostat.github.io/usdata) package.
```

```{block2, type="data", echo=TRUE}
The [`loan50`](http://openintrostat.github.io/openintro/reference/loan50.html) data can be found in the [openintro](http://openintrostat.github.io/openintro) package.
```

### Scatterplots for paired data {#scatterplots}

\index{data!loan50|(}

A **scatterplot** provides a case-by-case view of data for two numerical variables.
In Figure \@ref(fig:county-multi-unit-homeownership), a scatterplot was used to examine the homeownership rate against the fraction of housing units that were part of multi-unit properties (e.g. apartments) in the `county` data set.
Another scatterplot is shown in Figure \@ref(fig:loan50-amount-income), comparing the total income of a borrower `total_income` and the amount they borrowed `loan_amount` for the `loan50` data set.
In any scatterplot, each point represents a single case.
Since there are `r nrow(loan50)` cases in `loan50`, there are `r nrow(loan50)` points in Figure \@ref(fig:loan50-amount-income).

```{r include=FALSE}
terms_chp_2 <- c("scatterplot")
```

```{r loan50-amount-income, fig.cap = "A scatterplot of `loan_amount` versus `total_income` for the `loan50` data set.", warning=FALSE}
ggplot(loan50, aes(x = total_income, y = loan_amount)) +
  geom_point(alpha = 0.6, color = COL["blue", "full"], 
             fill = COL["blue", "full"], shape = 21, size = 3) +
  labs(x = "Total income", y = "Loan amount") +
  scale_x_continuous(labels = dollar_format(scale = 0.001, suffix = "K")) +
  scale_y_continuous(labels = dollar_format(scale = 0.001, suffix = "K"))
```

Looking at Figure \@ref(fig:loan50-amount-income), we see that there are many borrowers with income below \$100,000 on the left side of the graph, while there are a handful of borrowers with income above \$250,000.

```{r median-hh-income-poverty, fig.cap = "A scatterplot of the median household income against the poverty rate for the `county` dataset. Data are from 2017. A statistical model has also been fit to the data and is shown as a dashed line.", warning=FALSE, message=FALSE}
ggplot(county, aes(x = poverty/100, y = median_hh_income)) +
  geom_point(alpha = 0.3, color = COL["blue", "full"], 
             fill = COL["black", "full"], shape = 21, size = 3) +
  geom_smooth(linetype = "dashed", color = "gray", se = FALSE) +
  labs(x = "Poverty rate",y = "Median household income") +
  scale_x_continuous(labels = percent_format(accuracy = 1)) +
  scale_y_continuous(labels = dollar_format(scale = 0.001, suffix = "K"))
```


```{block2, type="example", echo=TRUE}
Figure \@ref(fig:median-hh-income-poverty) shows a plot of median household income against the poverty rate for `r nrow(county)` counties.
What can be said about the relationship between these variables?

---

The relationship is evidently **nonlinear**, as highlighted by the dashed line. This is different from previous scatterplots we have seen, which show relationships that do not show much, if any, curvature in the trend.
```

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "nonlinear")
```


```{block2, type="guidedpractice", echo=TRUE}
What do scatterplots reveal about the data, and how are they useful?^[Answers may vary. Scatterplots are helpful in quickly spotting associations relating variables, whether those associations come in the form of simple trends or whether those relationships are more complex.]
```

```{block2, type="guidedpractice", echo=TRUE}
Describe two variables that would have a horseshoe-shaped association in a scatterplot ($\cap$ or $\frown$)^[Consider the case where your vertical axis represents something "good" and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description: we require some water to survive, but consume too much and it become toxic and can kill a person.]
```


### Dot plots and the mean {#dotplots}

Sometimes we are interested in the distribution of a single variable. 
In these cases, a dot plot provides the most basic of displays.
A **dot plot** is a one-variable scatterplot; an example using the interest rate of `r nrow(loan50)` loans is shown in Figure \@ref(fig:loan-int-rate-dotplot).

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "dot plot")
```

```{r loan-int-rate-dotplot, fig.cap="A dot plot of `interest_rate` for the `loan50` dataset. The rates have been rounded to the nearest percent in this plot, and the distribution's mean is shown as a red triangle."}
loan50_interest_rate_mean <- mean(loan50$interest_rate)

ggplot(loan50, aes(x = interest_rate)) +
  geom_dotplot(fill = COL["blue", "full"], color = COL["blue", "full"]) +
  labs(x = "Interest rate") +
  scale_x_continuous(labels = label_percent(scale = 1)) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  geom_polygon(
    data = data.frame(x = c(loan50_interest_rate_mean - 1, loan50_interest_rate_mean + 1, loan50_interest_rate_mean), 
                      y = c(-0.1, -0.1, 0)),
    aes(x = x, y = y),
    fill = COL["red", "full"]
  )
```

The **mean**, often called the **average** is a common way to measure the center of a **distribution** of data. 
To compute the mean interest rate, we add up all the interest rates and divide by the number of observations.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "mean", "average", "distribution")
```

The sample mean is often labeled $\bar{x}$.
The letter $x$ is being used as a generic placeholder for the variable of interest and the bar over the $x$ communicates we're looking at the average interest rate, which for these 50 loans is `r round(loan50_interest_rate_mean, 2)`%.
It's useful to think of the mean as the balancing point of the distribution, and it's shown as a triangle in Figure \@ref(fig:loan-int-rate-dotplot).

```{block2, type="onebox", echo=TRUE}
**Mean.**
The sample mean can be calculated as the sum of the observed values divided by the number of observations:

\[ \bar{x} = \frac{x_1 + x_2 + \cdots + x_n}{n} \]
```

```{block2, type="guidedpractice", echo=TRUE}
Examine the equation for the mean. What does $x_1$ correspond to? And $x_2$ Can you infer a general meaning to what $n_i$ might represent?^[$x_1$ corresponds to the interest rate for the first loan in the sample, $x_2$ to the second loan's interest rate, and $x_i$ corresponds to the interest rate for the $i^{th}$ loan in the data set. For example, if $i = 4$, then we're examining $x_4$, which refers to the fourth observation in the data set.]
```

```{block2, type="guidedpractice", echo=TRUE}
What was $n$ in this sample of loans?^[The sample size was $n = 50$.]
```

The `loan50` data set represents a sample from a larger population of loans made through Lending Club.
We could compute a mean for this population in the same way as the sample mean.
However, the population mean has a special label: $\mu$.
The symbol $\mu$ is the Greek letter *mu* and represents the average of all observations in the population.
Sometimes a subscript, such as $_x$, is used to represent which variable the population mean refers to, e.g. $\mu_x$.
Often times it is too expensive to measure the population mean precisely, so we often estimate $\mu$ using the sample mean, $\bar{x}$.

```{block2, type="pronunciation", echo=TRUE}
The Greek letter $\mu$ is pronounced *mu*, listen to the pronunciation [here](https://youtu.be/PStgY5AcEIw?t=47).
```

```{block2, type="example", echo=TRUE}
The average interest rate across all loans in the population can be estimated using the sample data. Based on the sample of 50 loans, what would be a reasonable estimate of $\mu_x$, the mean interest rate for all loans in the full data set?

---
  
The sample mean, `r round(loan50_interest_rate_mean, 2)`, provides a rough estimate of $\mu_x$. While it is not perfect, this is our single best guess **point estimate**\index{point estimate} of the average interest rate of all the loans in the population under study. In Chapter \@ref(inference-foundations) and beyond, we will develop tools to characterize the accuracy of point estimates, like the sample mean. As you might have guessed, point estimates based on larger samples tend to be more accurate than those based on smaller samples.
```

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "point estimate")
```

The mean is useful because it allows us to rescale or standardize a metric into something more easily interpretable and comparable. 
Suppose we would like to understand if a new drug is more effective at treating asthma attacks than the standard drug. 
A trial of 1500 adults is set up, where 500 receive the new drug, and 1000 receive a standard drug in the control group:

```{r}
drug_asthma <- tribble(
  ~x,                     ~`New drug`, ~`Standard drug`,
  "Number of patients",   500,         1000,
  "Total asthma attacks", 200,         300
)
drug_asthma %>%
  kable(col.names = c("", "New drug", "Standard drug"),
        align = c("lcc"))
```

Comparing the raw counts of 200 to 300 asthma attacks would make it appear that the new drug is better, but this is an artifact of the imbalanced group sizes.
Instead, we should look at the average number of asthma attacks per patient in each group:

- New drug: $200 / 500 = 0.4$ asthma attacks per patient
- Standard drug: $300 / 1000 = 0.3$ asthma attacks per patient

The standard drug has a lower average number of asthma attacks per patient than the average in the treatment group.

```{block2, type="example", echo=TRUE}
Provide another examples where the mean is useful for making comparisons.

---
  
Emilio opened a food truck last year where he sells burritos, and his business has stabilized over the last 3 months.
Over that 3 month period, he has made $11,000 while working 625 hours.
Emilio's average hourly earnings provides a useful statistic for evaluating whether his venture is, at least from a financial perspective, worth it:

\[ \frac{\$11000}{625\text{ hours}} = \$17.60\text{ per hour} \]
  
By knowing his average hourly wage,
Emilio now has put his earnings into a standard unit that is easier to compare with many other jobs that he might consider.
```

```{block2, type="example", echo=TRUE}
Suppose we want to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes across the 3,142 counties in the \data{county} data set. What would be a better approach?

---
  
The `county` data set is special in that each county actually represents many individual people.
If we were to simply average across the `income` variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations.
Instead, we should compute the total income for each county, add up all the counties' totals, and then divide by the number of people in all the counties.
If we completed these steps with the \data{county} data, we would find that the per capita income for the US is $30,861.
Had we computed the *simple* mean of per capita income across counties, the result would have been just $26,093!

This example used what is called a **weighted mean**.
For more information on this topic, check out the following online supplement regarding [weighted means](openintro.org/d?file=stat_wtd_mean).
```


```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "weighted mean")
```

### Histograms and shape {#histograms}

Dot plots show the exact value for each observation.
This is useful for small data sets, but they can become hard to read with larger samples. 
Rather than showing the value of each observation, we prefer to think of the value as belonging to a *bin*.
For example, in the `loan50` data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on.
Observations that fall on the boundary of a bin (e.g. 10.00%) are allocated to the lower bin.
This tabulation is shown in Table \@ref(tab:binnedIntRateAmountTable).
These binned counts are plotted as bars in Figure \@ref(fig:loan50IntRateHist) into what is called a **histogram**, which resembles a more heavily binned version of the stacked dot plot shown in Figure \@ref(fig:loan-int-rate-dotplot).

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "histogram")
```

```{r binnedIntRateAmountTable}
loan50 %>%
  mutate(interest_rate_cat = cut(interest_rate, breaks = seq(5, 27.5, 2.5))) %>%
  count(interest_rate_cat, name = "Count") %>%
  separate(interest_rate_cat, into = c("lower", "upper"), sep = ",") %>%
  mutate(
    lower = str_remove(lower, "\\("),
    upper = str_remove(upper, "]"),
    lower = paste0(lower, "%"),
    upper = paste0(upper, "%")
  ) %>%
  unite("Interest rate", lower:upper, sep = " - ") %>%
  pivot_wider(names_from = `Interest rate`, values_from = Count) %>%
  bind_cols(tibble("Interest rate" = "Count"), .) %>%
  kable(align = "lccccccccc", caption = "Counts for the binned `interest_rate` data.")
```

```{r loan50IntRateHist, fig.cap = "A histogram of `interest_rate`. This distribution is strongly skewed to the right."}
ggplot(loan50, aes(x = interest_rate)) +
  geom_histogram(breaks = seq(5, 27.5, 2.5), fill = COL["blue", "full"]) +
  labs(x = "Interest rate", y = "Frequency") +
  scale_x_continuous(breaks = seq(5, 25, 5), labels = label_percent(scale = 1, accuracy = 1))
```

Histograms provide a view of the **data density**.
Higher bars represent where the data are relatively more common. 
For instance, there are many more loans with rates between 5% and 10% than loans with rates between 20% and 25% in the data set.
The bars make it easy to see how the density of the data changes relative to the interest rate.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "data density")
```

Histograms are especially convenient for understanding the shape of the data distribution.
Figure \@ref(fig:loan50IntRateHist) suggests that most loans have rates under 15\%, while only a handful of loans have rates above 20%.
When data trail off to the right in this way and has a longer right **tail**, the shape is said to be **right skewed**^[Other ways to describe data that are right skewed:
skewed to the right, skewed to the high end, or skewed to the positive end.]

Data sets with the reverse characteristic -- a long, thinner tail to the left -- are said to be **left skewed**.
We also say that such a distribution has a long left tail.
Data sets that show roughly equal trailing off in both directions are called **symmetric**.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "tail", "right skewed", "left skewed", "symmetric")
```

```{block2, type="onebox"}
When data trail off in one direction, the distribution has a **long tail**.
If a distribution has a long left tail, it is left skewed.
If a distribution has a long right tail, it is right skewed.
```

```{block2, type="guidedpractice"}
Besides the mean (since it was labeled), what can you see in the dot plot in Figure \@ref(fig:loan-int-rate-dotplot) that you cannot see in the histogram in Figure \@ref(fig:loan50IntRateHist)?^[The interest rates for individual loans.]
```

In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes.
A **mode** is represented by a prominent peak in the distribution.
There is only one prominent peak in the histogram of `interest_rate`.

A definition of *mode* sometimes taught in math classes is the value with the most occurrences in the data set.
However, for many real-world data sets, it is common to have *no* observations with the same value in a data set, making this definition impractical in data analysis.

Figure \@ref(fig:singleBiMultiModalPlots) shows histograms that
have one, two, or three prominent peaks.
Such distributions are called **unimodal**, **bimodal**, and **multimodal**, respectively.
Any distribution with more than 2~prominent peaks is called multimodal.
Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "unimodal", "bimodal", "multimodal")
```

```{r singleBiMultiModalPlots, fig.cap = "Counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal. Note that the left plot is unimodal because we are counting prominent peaks, not just any peak.", fig.asp=0.33, out.width = "90%", warning=FALSE}
df_modes <- tibble(
  uni   = rchisq(65, 6),
  bi    = c(rchisq(25, 5.8), rnorm(40, 20, 2)),
  multi =  c(rchisq(25, 3), rnorm(25, 15), rnorm(15, 25, 1.5))
)

p_uni   <- ggplot(df_modes, aes(x = uni)) + 
  geom_histogram(binwidth = 2, fill = COL["blue", "full"]) +
  labs(x = NULL, y = NULL) +
  ylim(0, 23) +
  xlim(0, 30)
p_bi    <- ggplot(df_modes, aes(x = bi)) + 
  geom_histogram(binwidth = 2, fill = COL["blue", "full"]) +
  labs(x = NULL, y = NULL) +
  ylim(0, 23) +
  xlim(0, 30)
p_multi <- ggplot(df_modes, aes(x = multi)) + 
  geom_histogram(binwidth = 2, fill = COL["blue", "full"]) +
  labs(x = NULL, y = NULL) +
  ylim(0, 23) +
  xlim(0, 30)

p_uni + p_bi + p_multi
```

```{block2, type="example", echo=TRUE}
Figure \@ref(loan50IntRateHist) reveals only one prominent mode in the interest rate. Is the distribution unimodal, bimodal, or multimodal?^[Unimodal Remember that *uni* stands for 1 (think *uni*cycles). Similarly, *bi* stands for 2 (think *bi*cycles). We are hoping a *multi*cycle will be invented to complete this analogy.]
```

```{block2, type="guidedpractice", echo=TRUE}
Height measurements of young students and adult teachers at a K-3 elementary school were taken.
How many modes would you expect in this height data set?^[There might be two height groups visible in the data set: one of the students and one of the adults. That is, the data are probably bimodal.].
```

Looking for modes isn't about finding a clear and correct answer about the number of modes in a distribution, which is why *prominent*\index{prominent} is not rigorously defined in this book. 
The most important part of this examination is to better understand your data.

### Variance and standard deviation (#variance-sd)

The mean was introduced as a method to describe the center of a data set, and **variability**\index{variability} in the data is also important.
Here, we introduce two measures of variability: the variance and the standard deviation.
Both of these are very useful in data analysis, even though their formulas are a bit tedious to calculate by hand.
The standard deviation is the easier of the two to comprehend, and it roughly describes how far away the typical observation is from the mean.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "variability")
```

We call the distance of an observation from its mean its **deviation**. Below are the deviations for the $1^{st}$, $2^{nd}$, $3^{rd}$, and $50^{th}$ observations in the `interest_rate` variable:

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "deviation")
```

```{r include=FALSE}
loan50_interest_rate_deviation <- round(loan50$interest_rate - loan50_interest_rate_mean, 2)
loan50_interest_rate_deviation_squared <- round(loan50_interest_rate_deviation^2, 2)
loan50_interest_rate_var <- var(loan50$interest_rate)
loan50_interest_rate_sd <- sd(loan50$interest_rate)
```

\[ x_1 - \bar{x} = `r round(loan50$interest_rate[1], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[1]` \]
\[ x_2 - \bar{x} = `r round(loan50$interest_rate[2], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[2]` \]
\[ x_3 - \bar{x} = `r round(loan50$interest_rate[3], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[3]` \]
\[ \vdots \]
\[ x_{50} - \bar{x} = `r round(loan50$interest_rate[50], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[50]` \]

If we square these deviations and then take an average, the result is equal to the sample **variance**, denoted by $s^2$:

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "variance")
```

\[ s^2 = \frac{(`r loan50_interest_rate_deviation[1]`)^2 + (`r loan50_interest_rate_deviation[2]`)^2 + (`r loan50_interest_rate_deviation[3]`)^2 + \cdots + (`r loan50_interest_rate_deviation[50]`)^2}{50 - 1} = \frac{`r loan50_interest_rate_deviation_squared[1]` + `r loan50_interest_rate_deviation_squared[2]` + \cdots + `r loan50_interest_rate_deviation_squared[50]`}{49} = `r round(loan50_interest_rate_var, 2)` \]

We divide by $n - 1$, rather than dividing by $n$, when computing a sample's variance; there's some mathematical nuance here, but the end result is that doing this makes this statistic slightly more reliable and useful.

Notice that squaring the deviations does two things.
First, it makes large values relatively much larger.
Second, it gets rid of any negative signs.

The **standard deviation** is defined as the square root of the variance:

\[ s = \sqrt{`r round(loan50_interest_rate_var, 2)`} = `r round(loan50_interest_rate_sd, 2)` \]

While often omitted, a subscript of $_x$ may be added to the variance and standard deviation,
i.e. $s_x^2$ and $s_x^{}$, if it is useful as a reminder that these are the variance and standard deviation of the observations represented by $x_1$, $x_2$, ..., $x_n$.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "standard deviation")
```

```{block2, type="onebox", echo=TRUE}
**Variance and standard deviation.**
The variance is the average squared distance from the mean.
The standard deviation is the square root of the variance.
The standard deviation is useful when considering how far the data are distributed from the mean.

The standard deviation represents the typical deviation of observations from the mean.
Usually about 70% of the data will be within one standard deviation of the mean and about 95\% will be within two standard deviations.
However, these percentages are not strict rules.
```

Like the mean, the population values for variance and standard deviation have special symbols: $\sigma^2$ for the variance and $\sigma$ for the standard deviation.

```{block2, type="pronunciation", echo=TRUE}
The Greek letter $\sigma$ is pronounced *sigma*, listen to the pronunciation [here](https://youtu.be/PStgY5AcEIw?t=72).
```

```{r}

#box_sd1 <- data.frame(x = loan50_interest_rate_mean - )

ggplot(loan50, aes(x = interest_rate)) +
  geom_dotplot(COL = COL["blue", "full"]) +
  labs(y = NULL, x = "Interest rate") +
  scale_x_continuous(labels = label_percent(scale = 1)) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  geom_polygon(
    data = data.frame(x = c(loan50_interest_rate_mean - 0.01, loan50_interest_rate_mean + 0.01, loan50_interest_rate_mean), 
                      y = c(-0.1, -0.1, 0)),
    aes(x = x, y = y),
    fill = COL["red", "full"]
  )
```

