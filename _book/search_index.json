[
["index.html", "Introduction to Statistics with Randomization and Simulation Welcome Textbook overview Examples, exercises, and additional appendices OpenIntro, online resources, and getting involved Acknowledgements", " Introduction to Statistics with Randomization and Simulation Mine Çetinkaya-Rundel, Johanna Hardin, David Diez, others… 2020-01-16 Welcome We hope readers will take away three ideas from this book in addition to forming a foundation of statistical thinking and methods. Statistics is an applied field with a wide range of practical applications. You don’t have to be a math guru to learn from interesting, real data. Data are messy, and statistical tools are imperfect. However, when you understand the strengths and weaknesses of these tools, you can use them to learn interesting things about the~world. IMPORTANT NOTE: This book is currently in active development, and is planned for launch in late 2020 as the 2nd edition of OpenIntro Statistics - Introduction to Statistics with Randomization and Simulation. The 1st edition of the book can be accessed at openintro.org/book/isrs/. Textbook overview Introduction to data. Data structures, variables, summaries, graphics, and basic data collection techniques. Exploratory data analysis. Data visualization and summarisation. Correlation and regression. Visualising relationships between many variables and descriptive summaries for quantifying the relationship between two variables. Multiple regression. Descriptive summaries for quantifying the relationship between two variables. Foundations for inference. Case studies are used to introduce the ideas of statistical inference with randomization and simulations. Inference for categorical data. Inference for proportions using simulation and randomization techniques as well as the normal and chi-square distributions. Inference for numerical data. Inference for one or two sample means using simulation and randomization techniques as well as the normal and F distributions. Inference for regression. Extending inference techniques presented thus-far to regression settings. Appendix: Probability. An introduction to probability is provided as an optional reference. Exercises and additional probability content may be found in Chapter~3 of OpenIntro Statistics at openintro.org/book/os. Examples, exercises, and additional appendices Examples and guided practice exercises throughout the textbook may be identified by their distinctive bullets: [MCR-TODO: Need to update language below] OpenIntro, online resources, and getting involved OpenIntro is an organization focused on developing free and affordable education materials. We encourage anyone learning or teaching statistics to visit [openintro.org] (http://www.openintro.org) and get involved. We also provide many free online resources, including free course software. Students can test their knowledge with practice quizzes for each chapter or try an application of concepts learned using real data. Data sets for this textbook are available on the website and through a companion R package, openintro.1 All of these resources are free, and we want to be clear that anyone is welcome to use these online tools and resources with or without this textbook as a companion. We value your feedback. If there is a part of the project you especially like or think needs improvement, we want to hear from you. You may find our contact form at openintro.org. Acknowledgements [MCR-TODO: Will need to update language and people mentioned here.] This project would not be possible without the dedication and volunteer hours of all those involved. No one has received any monetary compensation from this project, and we hope you will join us in extending a thank you to all those who volunteer with OpenIntro. The authors would especially like to thank Andrew Bray and Meenal Patel for their involvement and contributions to this textbook. We are also grateful to Andrew Bray, Ben Baumer, and David Laffie for providing us with valuable feedback based on their experiences while teaching with this textbook, and to the many teachers, students, and other readers who have helped improve OpenIntro resources through their feedback. The authors would like to specially thank George Cobb of Mount Holyoke College and Chris Malone of Winona State University. George has spent a good part of his career supporting the use of nonparametric techniques in introductory statistics, and Chris was helpful in discussing practical considerations for the ordering of inference used in this textbook. Thank you, George and Chris! Mine Çetinkaya-Rundel, David Diez, Andrew Bray, Albert Kim, Ben Baumer, Chester Ismay and Christopher Barr (2019). openintro: Data Sets and Supplemental Functions from ‘OpenIntro’ Textbooks and Labs. R package version 2.0.1. https://github.com/OpenIntroStat/openintro-r-package.↩ "],
["about-the-authors.html", "About the authors", " About the authors Mine Çetinkaya-Rundel mine@openintro.org University of Edinburgh, Duke University, RStudio Johanna Hardin jo@openintro.org Pomona College David Diez david@openintro.org Google/YouTube others… "],
["copyright.html", "Copyright", " Copyright Copyright © 2020. Second Edition. This textbook is available under a Creative Commons license. Visit openintro.org for a free PDF, to download the textbook’s source files, or for more information about the license. "],
["intro-to-data.html", "Chapter 1 Introduction to data 1.1 Case Study: using stents to prevent strokes 1.2 Taxonomy of Data 1.3 Overview of data collection principles 1.4 Observational studies and sampling strategies 1.5 Experimental design and causality 1.6 Data in R", " Chapter 1 Introduction to data .column-left{ float: left; width: 49%; text-align: left; } .column-right{ float: right; width: 49%; text-align: left; } Scientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data, and in this first chapter, we focus on both the properties of data and on the collection of data. Identify a question or problem. Collect relevant data on the topic. Analyze the data. Form a conclusion. Statistics as a subject focuses on making stages 2-4 objective, rigorous, and efficient. That is, statistics has three primary components: How best can we collect data? How should it be analyzed? And what can we infer from the analysis? The topics scientists investigate are as diverse as the questions they ask. However, many of these investigations can be addressed with a small number of data collection techniques, analytic tools, and fundamental concepts in statistical inference. This chapter provides a glimpse into these and other themes we will encounter throughout the rest of the book. We introduce the basic principles of each branch and learn some tools along the way. We will encounter applications from other fields, some of which are not typically associated with science but nonetheless can benefit from statistical study. 1.1 Case Study: using stents to prevent strokes Section 1.1 introduces a classic challenge in statistics: evaluating the efficacy of a medical treatment. Terms in this section, and indeed much of this chapter, will all be revisited later in the text. The plan for now is simply to get a sense of the role statistics can play in practice. In this section we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke.2 Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer: Does the use of stents reduce the risk of stroke? The researchers who asked this question conducted an experiment with 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups: Treatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification. Control group. Patients in the control group received the same medical management as the treatment group, but they did not receive stents. Researchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group. Researchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. The results of 5 patients are summarized in Table 1.1. Patient outcomes are recorded as stroke or no event, representing whether or not the patient had a stroke at the end of a time period. Table 1.1: Results for five patients from the stent study. patient group 0-30 days 0-365 days 1 treatment no event no event 2 control no event stroke 3 control no event no event 4 control no event stroke 5 control no event no event Considering data from each patient individually would be a long, cumbersome path towards answering the original research question. Instead, performing a statistical data analysis allows us to consider all of the data at once. Table 1.1 summarizes the raw data in a more helpful way. In this table, we can quickly see what happened over the entire study. For instance, to identify the number of patients in the treatment group who had a stroke within 30 days, we look on the left-side of the table at the intersection of the treatment and stroke: 33. Table 1.2: Descriptive statistics for the stent study. group 0-30 days_stroke 0-30 days_no event 0-365 days_stroke 0-365 days_no event treatment 33 191 45 179 control 13 214 28 199 [MCR-TODO: Figure out LaTeX exercise counter stuff that’s differently formatted for in text exercises.] Exercise: see 1.1 Exercise 1.1 (Proportion of stroke victims) Of the 224 patients in the treatment group, 45 had a stroke by the end of the first year. Using these two numbers, compute the proportion of patients in the treatment group who had a stroke by the end of their first year. (Please note: answers to all Guided Practice exercises are provided using footnotes.)3 We can compute summary statistics from the table. A summary statistic is a single number summarizing a large amount of data. For instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups. Proportion who had a stroke in the treatment (stent) group: \\(45/224 = 0.20 = 20\\%\\). Proportion who had a stroke in the control group: \\(28/227 = 0.12 = 12\\%\\). These two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 8% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a “real” difference between the groups? This second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 8% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance? While we don’t yet have our statistical tools to fully address this question on our own, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients. Be careful: Do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises. 1.2 Taxonomy of Data Effective presentation and description of data is a first step in most analyses. This section introduces one structure for organizing data as well as some terminology that will be used throughout this book. 1.2.1 Observations, variables, and data matrices Table 1.3 displays the first six rows of a data set concerning 50 emails received in 2012. These observations will be referred to as the data set, and they are a random sample from a larger data set that we will see in Section 7.2. Each row in the table represents a single email or case.4 The columns represent characteristics, called variables, for each of the emails. For example, the first row represents email 1, which is not spam, contains 21,705 characters, 551 line breaks, is written in HTML format, and contains only small numbers. In practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and the units of measurement. Descriptions of all five email variables are given in Table 1.4. Table 1.3: Six rows from the email50 data matrix. spam num_char line_breaks format number 0 21.705 551 1 small 0 7.011 183 1 big 1 0.631 28 0 none 0 2.454 61 0 small 0 41.623 1088 1 small 0 0.057 5 0 small [MCR-TODO: Unbelievably annoying that I can’t print the help file. Do you know how?? Otherwise, we have to leave this as a markdown table.] Table 1.4: Variables and their descriptions for the email50 data set. variable description spam Indicator for whether the email was spam. num_char The number of characters in the email, in thousands. line_breaks The number of line breaks in the email (does not count text wrapping). format Indicates if the email contained special formatting, such as bolding, tables, or links, which would indicate the message is in HTML format number Indicates whether the email contained no number, a small number (under 1 million), or a large number The data in Table 1.3 represent a data matrix, which is a common way to organize data. Each row of a data matrix corresponds to a unique case, and each column corresponds to a variable. A data matrix for the stroke study introduced in Section 1.1 is shown in Table 1.1, where the cases were patients and there were three variables recorded for each patient. Data matrices are a convenient way to record and store data. If another individual or case is added to the data set, an additional row can be easily added. Similarly, another column can be added for a new variable. Exercise 1.2 We consider a publicly available data set that summarizes information about the 3,143 counties in the United States, and we call this the data set. This data set includes information about each county: its name, the state where it resides, its population in 2000 and 2010, per capita federal spending, poverty rate, and five additional characteristics. How might these data be organized in a data matrix? Reminder: look in the footnotes for answers to in-text exercises.5 Seven rows of the data set are shown in Table 1.5, and the variables are summarized in Table 1.6. These data were collected from the US Census website.6 Table 1.5: Seven rows from the county data set. name state pop2000 pop2010 fed_spend poverty homeownership multiunit income med_income Autauga County Alabama 43671 54571 6.07 10.6 77.5 7.2 24568 53255 Baldwin County Alabama 140415 182265 6.14 12.2 76.7 22.6 26469 50147 Barbour County Alabama 29038 27457 8.75 25.0 68.0 11.1 15875 33219 Bibb County Alabama 20826 22915 7.12 12.6 82.9 6.6 19918 41770 Blount County Alabama 51024 57322 5.13 13.4 82.0 3.7 21070 45549 Bullock County Alabama 11714 10914 9.97 25.3 76.9 9.9 20289 31602 Butler County Alabama 21399 20947 9.31 25.0 69.0 13.7 16916 30659 Table 1.6: Variables and their descriptions for the county data set. variable description name County name state State where the county resides (also including the District of Columbia) pop2000 Population in 2000 pop2010 Population in 2010 fed_spend Federal spending per capita poverty Percent of the population in poverty homeownership Percent of the population that lives in their own home or lives with the owner (e.g. children living with parents who own the home) multiunit Percent of living units that are in multi-unit structures (e.g. apartments) income Income per capita med_income Median household income for the county, where a household’s income equals the total income of its occupants who are 15 years or older smoking_ban Type of county-wide smoking ban in place at the end of 2011, which takes one of three values: none, partial, or comprehensive, where a comprehensive ban means smoking was not permitted in restaurants, bars, or workplaces, and partial means smoking was banned in at least one of those three locations 1.2.2 Types of variables Examine the fed_spend, pop2010, state, and smoking_ban variables in the data set. Each of these variables is inherently different from the other three yet many of them share certain characteristics. First consider fed_spend, which is said to be a numerical variable since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as numerical since their average, sum, and difference have no clear meaning. The pop2010 variable is also numerical, although it seems to be a little different than fed_spend. This variable of the population count can only be a whole non-negative number (0, 1, 2 …). For this reason, the population variable is said to be discrete since it can only take numerical values with jumps. On the other hand, the federal spending variable is said to be continuous. The variable state can take up to 51 values after accounting for Washington, DC: AL, …, and WY. Because the responses themselves are categories, state is called a categorical variable,7 and the possible values are called the variable’s levels. Figure 1.1: Breakdown of variables into their respective types. Finally, consider the smoking_ban variable, which describes the type of county-wide smoking ban and takes a value none,partial, or comprehensive in each county. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable. To simplify analyses, any ordinal variables in this book will be treated as categorical variables. Example 1.1 Data were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous numerical, discrete numerical, or categorical. The number of siblings and student height represent numerical variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous numerical variable. The last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical. Exercise 1.3 Consider the variables group and outcome (at 30 days) from the stent study in Section 1.1. Are these numerical or categorical variables?8 1.2.3 Relationships between variables Many analyses are motivated by a researcher looking for a relationship between two or more variables. A social scientist may like to answer some of the following questions: Is federal spending, on average, higher or lower in counties with high rates of poverty? If homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county likely be above or below the national average? Which counties have a higher average income: those that enact one or more smoking bans or those that do not? To answer these questions, data must be collected, such as the data set shown in Table 1.5. Examining summary statistics summary statistics could provide insights for each of the three questions about counties. Additionally, graphs can be used to visually summarize data and are useful for answering such questions as well. Scatterplots are one type of graph used to study the relationship between two numerical variables. Figure 1.2 compares the variables fed_spend and poverty. Each point on the plot represents a single county. For instance, the highlighted dot corresponds to County 1088 in the data set: Owsley County, Kentucky, which had a poverty rate of 41.5% and federal spending of $21.50 per capita. The dense cloud in the scatterplot suggests a relationship between the two variables: counties with a high poverty rate also tend to have slightly more federal spending. We might brainstorm as to why this relationship exists and investigate each idea to determine which is the most reasonable explanation. Figure 1.2: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted. Exercise 1.4 Examine the variables in the data set, which are described in Table 1.4. Create two questions about the relationships between these variables that are of interest to you.9 The fed_spend and poverty variables are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa. Example 1.2 The relationship between the homeownership rate and the percent of units in multi-unit structures (e.g. apartments, condos) is visualized using a scatterplot in Figure 1.3. Are these variables associated? It appears that the larger the fraction of units in multi-unit structures, the lower the homeownership rate. Since there is some relationship between the variables, they are associated. Figure 1.3: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties. Interested readers may find an image of this plot with an additional third variable, county population, presented at http://www.openintro.org/stat/down/MHP.png. Because there is a downward trend in Figure 1.3 – counties with more units in multi-unit structures are associated with lower homeownership – these variables are said to be negatively associated. A positive association is shown in the relationship between the poverty and fed_spend variables represented in Figure 1.2, where counties with higher poverty rates tend to receive more federal spending per capita. If two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two. 1.3 Overview of data collection principles The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals. 1.3.1 Populations and samples Consider the following three research questions: What is the average mercury content in swordfish in the Atlantic Ocean? Over the last 5 years, what is the average time to complete a degree for Duke undergraduate students? Does a new drug reduce the number of deaths in patients with severe heart disease? Each research question refers to a target population. In the first question, the target population is all swordfish in the Atlantic ocean, and each fish represents a case. It is usually too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question. Exercise 1.5 For the second and third questions above, identify the target population and what represents an individual case.10 1.3.2 Anecdotal evidence Consider the following possible responses to the three research questions: A man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high. I met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges. My friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work. Each conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence. Figure 1.4: In February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, “It is one storm, in one region, of one country.” Anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population. 1.3.3 Sampling from a population We might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. Figure 1.5: In this graphic, five graduates are randomly selected from the population to be included in the sample. Why pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario. Example 1.3 Suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might collect? Do you think her sample would be representative of all graduates? Perhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if that bias is unintentional or difficult to discern. Figure 1.6: Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often. If someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces bias into a sample. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample. The act of taking a simple random sample helps minimize bias, however, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, and it is unclear whether the respondents are representative of the entire population, the survey might suffer from non-response bias. Figure 1.7: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often impossible, to completely fix this problem. Another common downfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample. For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents. Exercise 1.6 We can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?11 1.3.4 Explanatory and response variables Consider the following question for the data set: Is federal spending, on average, higher or lower in counties with high rates of poverty? If we suspect poverty might affect spending in a county, then poverty is the explanatory variable and federal spending is the response variable in the relationship.12 If there are many variables, it may be possible to consider a number of them as explanatory variables. \\begin{tipBox}{ To identify the explanatory variable in a pair of variables, identify which of the two is suspected of affecting the other. \\end{tipBox} In some cases, there is no explanatory or response variable. Consider the following question: If homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county likely be above or below the national average? It is difficult to decide which of these variables should be considered the explanatory and response variable, i.e. the direction is ambiguous, so no explanatory or response labels are suggested here. 1.3.5 Introducing observational studies and experiments There are two primary types of data collection: observational studies and experiments. Researchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe what happens. In general, observational studies can provide evidence of a naturally occurring association between variables, but they cannot by themselves show a causal connection. When researchers want to investigate the possibility of a causal connection, they conduct an experiment. Usually there will be both an explanatory and a response variable. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a group, the experiment is called a randomized experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. See the case study in Section 1.1 for another example of an experiment, though that study did not employ a placebo. 1.4 Observational studies and sampling strategies 1.4.1 Observational studies Generally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations. Exercise 1.7 Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?13 Some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation. Sun exposure is what is called a confounding variable,14 which is a variable that is correlated with both the explanatory and response variables. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured. In the same way, the data set is an observational study with confounding variables, and its data cannot easily be used to make causal conclusions. Exercise 1.8 Figure 1.3 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest one or more other variables that might explain the relationship in Figure 1.3.15 Observational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of similar individuals over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses’ Health Study, started in 1976 and expanded in 1989.16 This prospective study recruits registered nurses and then collects data from them using questionnaires. Retrospective studies collect data after events have taken place, e.g. researchers may review past events in medical records. Some data sets, such as , may contain both prospectively- and retrospectively-collected variables. Local governments prospectively collect some variables as events unfolded (e.g. retails sales) while the federal government retrospectively collected others during the 2010 census (e.g. county population). 1.4.2 Three sampling methods (special topic) Almost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, results from these statistical methods are not reliable. Here we consider three random sampling techniques: simple, stratified, and cluster sampling. Figure 1.8 provides a graphical representation of these techniques. Figure 1.8: Examples of simple random, stratified, and cluster sampling. In the top panel, simple random sampling was used to randomly select the 18 cases. In the middle panel, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum. In the bottom panel, cluster sampling was used, where data were binned into nine clusters, and three of the clusters were randomly selected. Simple random sampling is probably the most intuitive form of random sampling. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries from the 2010 season, we could write the names of that season’s 828 players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included. Stratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, the teams could represent the strata; some teams have a lot more money (we’re looking at you, Yankees). Then we might randomly sample 4 players from each team for a total of 120 players. Stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling. Example 1.4 Why would it be good for cases within each stratum to be very similar? We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population. In cluster sampling, we group observations into clusters, then randomly sample some of the clusters. Sometimes cluster sampling can be a more economical technique than the alternatives. Also, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. For example, if neighborhoods represented clusters, then this sampling method works best when the neighborhoods are very diverse. A downside of cluster sampling is that more advanced analysis techniques are typically required, though the methods in this book can be extended to handle such data. Example 1.5 Suppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. What sampling method should be employed? A simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling seems like a very good idea. We might randomly select a small number of villages. This would probably reduce our data collection costs substantially in comparison to a simple random sample and would still give us helpful information. Another technique called multistage sampling is similar to cluster sampling, except that we take a simple random sample within each selected cluster. For instance, if we sampled neighborhoods using cluster sampling, we would next sample a subset of homes within each selected neighborhood if we were using multistage sampling. 1.5 Experimental design and causality Studies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. 1.5.1 Principles of experimental design Randomized experiments are generally built on four principles. Controlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill. Randomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study. Replication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. Figure 1.9: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly divided into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories. Blocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 1.9. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients. It is important to incorporate the first three experimental design principles into any study, and this book describes methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this book may be extended to analyze data collected using blocking. 1.5.2 Reducing bias in human experiments Randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationships in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients.17 In particular, researchers wanted to know if the drug reduced deaths in patients. These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers18 were randomly placed into two study groups. One group, the treatment group, received the drug. The other group, called the control group, did not receive any drug treatment. Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify. Researchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesn’t receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect. The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.19 Exercise 1.9 Look back to the study in Section 1.1 where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?20 1.6 Data in R skimr R is a powerful and open source software tool for working with data. Throughout this text, we provide some guidance on how to use R within the context of the statistical content that is being covered. As educators, we see the value of teaching with modern software in order to empower students to take optimal advantage of the concepts they are learning. However, we understand the limitations of some educational structures, and we know that not every classroom will be able to implement R alongside the statistical concepts. Generally, we will present the R techniques at the end of each chapter. There are times in the text when the concepts are not distinguishable from the software, and in those cases, we have have provided the R code within the main body of the chapter. We start with an introduction to R focused on how datasets are structured in R and how the user can work with a data object. 1.6.1 Dataframes in R Throughout the text, we will work with many different datasets. Some datasets are pre-loaded into R, some get loaded through R packages, and some datasets will be created by the student. Dataset can be viewed through the RStudio envionment, but the data can also be investigated through the notebook features of an RMarkdown file. Consider the data that was described previously in this chapter. We can glimpse the data to see the variables, or we can apply head to see the first few rows of the dataset. library(openintro) data(email50) glimpse(email50) #&gt; Observations: 50 #&gt; Variables: 21 #&gt; $ spam [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, … #&gt; $ to_multiple [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … #&gt; $ from [3m[90m&lt;dbl&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #&gt; $ cc [3m[90m&lt;int&gt;[39m[23m 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ sent_email [3m[90m&lt;dbl&gt;[39m[23m 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, … #&gt; $ time [3m[90m&lt;dttm&gt;[39m[23m 2012-01-04 05:19:16, 2012-02-16 12:10:06, 2012-01-… #&gt; $ image [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ attach [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, … #&gt; $ dollar [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, 0,… #&gt; $ winner [3m[90m&lt;fct&gt;[39m[23m no, no, no, no, no, no, no, no, no, no, no, no, yes… #&gt; $ inherit [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ viagra [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ password [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, … #&gt; $ num_char [3m[90m&lt;dbl&gt;[39m[23m 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809, … #&gt; $ line_breaks [3m[90m&lt;int&gt;[39m[23m 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167, … #&gt; $ format [3m[90m&lt;dbl&gt;[39m[23m 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, … #&gt; $ re_subj [3m[90m&lt;dbl&gt;[39m[23m 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, … #&gt; $ exclaim_subj [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ urgent_subj [3m[90m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ exclaim_mess [3m[90m&lt;dbl&gt;[39m[23m 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, 10… #&gt; $ number [3m[90m&lt;fct&gt;[39m[23m small, big, none, small, small, small, small, small… head(email50) %&gt;% kable() spam to_multiple from cc sent_email time image attach dollar winner inherit viagra password num_char line_breaks format re_subj exclaim_subj urgent_subj exclaim_mess number 0 0 1 0 1 2012-01-04 05:19:16 0 0 0 no 0 0 0 21.705 551 1 1 0 0 8 small 0 0 1 0 0 2012-02-16 12:10:06 0 0 0 no 0 0 0 7.011 183 1 0 0 0 1 big 1 0 1 4 0 2012-01-04 07:36:23 0 2 0 no 0 0 0 0.631 28 0 0 0 0 2 none 0 0 1 0 0 2012-01-04 09:49:52 0 0 0 no 0 0 0 2.454 61 0 0 0 0 1 small 0 0 1 0 0 2012-01-27 01:34:45 0 0 9 no 0 0 1 41.623 1088 1 0 0 0 43 small 0 0 1 0 0 2012-01-17 09:31:57 0 0 0 no 0 0 0 0.057 5 0 0 0 0 0 small Sometimes it is necessary to pull out one column or one entry from a data set. The $ operator will pull out a column, and the square brackets will index a dataset like a matrix. For example data[i,j] will produce the \\((i,j)^{th}\\) intry of data, and data[i,] will produce the \\(i^{th}\\) row. email50$num_char #&gt; [1] 21.705 7.011 0.631 2.454 41.623 0.057 0.809 5.229 9.277 17.170 #&gt; [11] 64.401 10.368 42.793 0.451 29.233 9.794 2.139 0.130 4.945 11.533 #&gt; [21] 5.682 6.768 0.086 3.070 26.520 26.255 5.259 2.780 5.864 9.928 #&gt; [31] 25.209 6.563 24.599 25.757 0.409 11.223 3.778 1.493 10.613 0.493 #&gt; [41] 4.415 14.156 9.491 24.837 0.684 13.502 2.789 1.169 8.937 15.829 email50[47,3] #&gt; [1] 1 email[47,] %&gt;% kable() spam to_multiple from cc sent_email time image attach dollar winner inherit viagra password num_char line_breaks format re_subj exclaim_subj urgent_subj exclaim_mess number 47 0 1 1 2 0 2012-01-02 13:24:21 0 0 0 no 0 0 0 8.72 185 0 1 0 0 3 small 1.6.2 Tidy Structure of Data For plotting, analyses, model building, etc., the data should be structured according to certain principles. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in (???). Tidy Data: rows (cases/observational units) and columns (variables). The key is that every row is a case and every column is a variable. No exceptions. Creating tidy data is often not trivial. Within R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language. Some things to consider: object_name &lt;- anything is a way of assigning anything to the new object_name. object_name &lt;- function_name(data_table, arguments) is a way of using a function to create a new object. object_name &lt;- data_table %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side. object_name &lt;- data_table %&gt;% function_name(arguments) %&gt;% another_function_name(other_arguments) is extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. * The pipe syntax should be read as then, %&gt;%. 1.6.3 Using the pipe to chain The pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example: x %&gt;% f(y) is the same as f(x, y) y %&gt;% f(x, ., z) is the same as f(x,y,z) Pipes are used commonly with functions in the dplyr package (see R examples in Chapter @ref{eda}) and they allow us to sequentially build data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations. Consider the data, High School and Beyond survey. Two hundred observations were randomly sampled from the High School and Beyond survey, a survey conducted on high school seniors by the National Center of Education Statistics. Of interest is the proportion of students at each of the two types of school, public and privaate. We use the table command to tabulate how many of each type of school are in the dataset. Notice that the same result is produced by the $ command with table and the chaining syntax done with %&gt;%. data(hsb2) table(hsb2$schtyp) #&gt; #&gt; public private #&gt; 168 32 hsb2 %&gt;% select(schtyp) %&gt;% table() #&gt; . #&gt; public private #&gt; 168 32 What if we are interested only in public schools? First, we should take note of another piece of R syntax: the double equal sign. This is the logical test for “is equal to”. In other words, we first determine if school type is equal to public for each of the observations in the dataset and filter for those where this is true. # Filter for public schools hsb2_public &lt;- hsb2 %&gt;% filter(schtyp == &quot;public&quot;) We can read this as: “take the hsb2 data frame and pipe it into the filter function. Filter the data for cases where school type is equal to public. Then, assign the resulting data frame to a new object called hsb2 underscore public.” Suppose we are not interested in the actual reading score of students, but instead whether their reading score is below average or at or above average. First, we need to calculate the average reading score with the mean function. This will give us the mean value, 52.23. However, in order to be able to refer back to this value later on, we might want to store it as an object that we can refer to by name. # Calculate average reading score and show the value mean(hsb2$read) #&gt; [1] 52.2 So instead of just printing the result, let’s save it as a new object called avg_read. # Calculate average reading score and store as avg_read avg_read &lt;- mean(hsb2$read) Before we more on, a quick tip: most often you’ll want to do both; see the value and also store it for later use. The approach we used here, running the mean function twice, is redundant. Instead, you can simply wrap your assignment code in parentheses so that R will not only assign the average value of reading test scores to avg read, but it will also print out its value. # Do both (avg_read &lt;- mean(hsb2$read)) #&gt; [1] 52.2 Next we need to determine whether each student is below or at or above average. For example, a reading score of 57 is above average, so is 68, but 44 is below. Obviously, going through each record like this would be tedious and error prone. Instead we can create this new variable with the mutate function from the dplyr package. We start with the data frame, hsb2, and pipe it into mutate, to create a new variable called read_cat (cat for categorical). Note that we are using a new variable name here in order to not overwrite the existing reading score variable. The new variable read_cat will be a column in the existing data frame hsb2. To indicate that the mutate function came from the dplyr package, we use the pacakge::function syntax. It is not usually necessary to provide the package name (unless there is ambiguity about where the function came from). hsb2 &lt;- hsb2 %&gt;% dplyr::mutate(read_cat = ifelse(read &lt; avg_read, &quot;below average&quot;, &quot;at or above average&quot;)) The decision criteria for this new variable is based on a TRUE/FALSE question: if the reading score of the student is below the average reading score, label “below average”, otherwise, label “at or above average”. This can be accomplished using the ifelse function in R. The first argument of the function is the logical test. The second argument is what to do if the result of the logical test is TRUE, in other words, if the student’s score is below the average score, and the last argument is what to do if the result is FALSE. The ifelse function can be used for more complicated discretization rules as well, by nesting many ifelse statements within each other. This is not necessary for this example, but it will come up later in the course. Chimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Medical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993-1003. http://www.nejm.org/doi/full/10.1056/NEJMoa1105335. NY Times article reporting on the study: http://www.nytimes.com/2011/09/08/health/research/08stent.html.↩ The proportion of the 224 patients who had a stroke within 365 days: \\(45/224 = 0.20\\).↩ A case is also sometimes called a unit of observation or an observational unit.↩ Each county may be viewed as a case, and there are eleven pieces of information recorded for each case. A table with 3,143 rows and 11 columns could hold these data, where each row represents a county and each column represents a particular piece of information.↩ http://quickfacts.census.gov/qfd/index.html↩ Sometimes also called a nominal variable.↩ There are only two possible values for each variable, and in both cases they describe categories. Thus, each is a categorical variable.↩ Two sample questions: (1) Intuition suggests that if there are many line breaks in an email then there would also tend to be many characters: does this hold true? (2) Is there a connection between whether an email format is plain text (versus HTML) and whether it is a spam message?↩ (2) Notice that the second question is only relevant to students who complete their degree; the average cannot be computed using a student who never finished her degree. Thus, only Duke undergraduate students who have graduated in the last five years represent cases in the population under consideration. Each such student would represent an individual case. (3) A person with severe heart disease represents a case. The population includes all people with severe heart disease.↩ Answers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, since our experiences may not be representative, we also keep an open mind.↩ Sometimes the explanatory variable is called the independent variable and the response variable is called the dependent variable. However, this becomes confusing since a pair of variables might be independent or dependent, so we avoid this language.↩ No. See the paragraph following the exercise for an explanation.↩ Also called a lurking variable, confounding factor, or a confounder.↩ Answers will vary. Population density may be important. If a county is very dense, then a larger fraction of residents may live in multi-unit structures. Additionally, the high density may contribute to increases in property value, making homeownership infeasible for many residents.↩ http://www.channing.harvard.edu/nhs/↩ Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.↩ Human subjects are often called patients, volunteers, or study participants.↩ There are always some researchers in the study who do know which patients are receiving which treatment. However, they do not interact with the study’s patients and do not tell the blinded health care professionals who is receiving which treatment.↩ The researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind since it was not blind.↩ "],
["droplevels.html", "Chapter 2 droplevels() #???", " Chapter 2 droplevels() #??? "],
["srs-using-sample-n.html", "Chapter 3 SRS using sample_n", " Chapter 3 SRS using sample_n "],
["stratified-sampling-using-group-by-then-sample-n.html", "Chapter 4 stratified sampling using group_by then sample_n", " Chapter 4 stratified sampling using group_by then sample_n "],
["r-code-for-blocking.html", "Chapter 5 R code for blocking???", " Chapter 5 R code for blocking??? "],
["add-case-when.html", "Chapter 6 add case_when?", " Chapter 6 add case_when? "],
["eda.html", "Chapter 7 Exploratory Data Analysis 7.1 Numerical Data 7.2 Categorical Data", " Chapter 7 Exploratory Data Analysis 7.1 Numerical Data [MCR-TODO: MCR wrote these bullet points, are they addressed?] Mention univariate - center, skew, shape, spread Mention conditional probabilities as well This section introduces techniques for exploring and summarizing numerical variables, and the and data sets from Section 1.2 provide rich opportunities for examples. Recall that outcomes of numerical variables are numbers on which it is reasonable to perform basic arithmetic operations. For example, the pop2010 variable, which represents the populations of counties in 2010, is numerical since we can sensibly discuss the difference or ratio of the populations in two counties. On the other hand, area codes and zip codes are not numerical. 7.1.1 Scatterplots for paired data A scatterplot provides a case-by-case view of data for two numerical variables. In Figure 1.2, a scatterplot was used to examine how federal spending and poverty were related in the data set. Another scatterplot is shown in Figure 7.1, comparing the number of line breaks (line_breaks) and number of characters (num_char) in emails for the data set. In any scatterplot, each point represents a single case. Since there are 50 cases in , there are 50 points in Figure 7.1. Figure 7.1: A scatterplot of line_breaks versus num_char for the email50 data. To put the number of characters in perspective, this paragraph has 363 characters. Looking at Figure 7.1, it seems that some emails are incredibly long! Upon further investigation, we would actually find that most of the long emails use the HTML format, which means most of the characters in those emails are used to format the email rather than provide text. Exercise 7.1 What do scatterplots reveal about the data, and how might they be useful?21 Example 7.1 Consider a new data set of 54 cars with two variables: vehicle price and weight.22 A scatterplot of vehicle price versus weight is shown in Figure 7.2. What can be said about the relationship between these variables? The relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots we’ve seen, such as Figure 1.2 and Figure 7.1, which show relationships that are very linear. Figure 7.2: A scatterplot of price versus weight for 54 cars. Exercise 7.2 Describe two variables that would have a horseshoe shaped association in a scatterplot.23 7.1.2 Dot plots and the mean Sometimes two variables is one too many: only one variable may be of interest. In these cases, a dot plot provides the most basic of displays. A dot plot is a one-variable scatterplot; an example using the number of characters from 50 emails is shown in Figure 7.3. A stacked version of this dot plot is shown in Figure 7.4. Figure 7.3: A dot plot of num_char for the email50 data set. Figure 7.4: A stacked dot plot of num_char for the email50 data set. The mean, sometimes called the average, is a common way to measure the center of a distribution of data. To find the mean number of characters in the 50 emails, we add up all the character counts and divide by the number of emails. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal. \\[\\begin{equation} \\bar{x} = \\frac{21.7 + 7.0 + \\cdots + 15.8}{50} = 11.6 \\tag{7.1} \\end{equation}\\] The sample mean is often labeled \\(\\bar{x}\\), and the letter \\(x\\) is being used as a generic placeholder for the variable of interest, num_char. The sample mean is shown as a triangle in Figures 7.3 and 7.4. –&gt; Exercise 7.3 Examine Equations (7.1) and (??) above. What does \\(x_1\\) correspond to? And \\(x_2\\)? Can you infer a general meaning to what \\(x_i\\) might represent?24 Exercise 7.4 What was \\(n\\) in this sample of emails?25 The data set is a sample from a larger population of emails that were received in January and March. We could compute a mean for this population in the same way as the sample mean. However, there is a difference in notation: the population mean has a special label: The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x\\), is used to represent which variable the population mean refers to, e.g. \\(\\mu_x\\). Example 7.2 The average number of characters across all emails can be estimated using the sample data. Based on the sample of 50 emails, what would be a reasonable estimate of \\(\\mu_x\\), the mean number of characters in all emails in the data set? (Recall that is a sample from .) The sample mean, 11,600, may provide a reasonable estimate of \\(\\mu_x\\). While this number will not be perfect, it provides a point estimate of the population mean. In Chapter ?? and beyond, we will develop tools to characterize the accuracy of point estimates, and we will find that point estimates based on larger samples tend to be more accurate than those based on smaller samples. Example 7.3 We might like to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes from the 3,143 counties in the data set. What would be a better approach? The data set is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. If we completed these steps with the data, we would find that the per capita income for the US is $27,348.43. Had we computed the simple mean of per capita income across counties, the result would have been just $22,504.70! Example 7.3 used what is called a weighted mean, which will not be a key topic in this textbook. However, we have provided an online supplement on weighted means for interested readers: http://www.openintro.org/stat/down/supp/wtdmean.pdf 7.1.3 Histograms and shape Dot plots show the exact value of each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, think of the value as belonging to a bin. For example, in the data set, we create a table of counts for the number of cases with character counts between 0 and 5,000, then the number of cases between 5,000 and 10,000, and so on. Observations that fall on the boundary of a bin (e.g. 5,000) are allocated to the lower bin. This tabulation is shown in Table 7.1. These binned counts are plotted as bars in Figure 7.5 into what is called a histogram, which resembles the stacked dot plot shown in Figure 7.4. Table 7.1: The counts for the binned num_char data. Characters (in thousands) 0-5 5-10 10-15 15-20 20-25 25-30 \\(\\cdots\\) 55-60 60-65 Count 19 12 6 2 3 5 \\(\\cdots\\) 0 1 Figure 7.5: A histogram of num_char. This distribution is very strongly skewed to the right. Histograms provide a view of the data density. Higher bars represent where the data are relatively more dense. For instance, there are many more emails between 0 and 10,000 characters than emails between 10,000 and 20,000 characters in the data set. The bars make it easy to see how the density of the data changes relative to the number of characters. Histograms are especially convenient for describing the shape of the data distribution. Figure 7.5 shows that most emails have a relatively small number of characters, while fewer emails have a very large number of characters. When data trail off to the right in this way and have a longer right tail, the shape is said to be right skewed.26 Data sets with the reverse characteristic – a long, thin tail to the left – are said to be left skewed. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called symmetric. Exercise 7.5 Take a look at the dot plots in Figures 7.3 and 7.4. Can you see the skew in the data? Is it easier to see the skew in this histogram or the dot plots?27 Exercise 7.6 Besides the mean (since it was labeled), what can you see in the dot plots that you cannot see in the histogram?28 In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution.29 There is only one prominent peak in the histogram of num_char. Figure 7.6 shows histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than 2 prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations. Figure 7.6: Counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal. Exercise 7.7 Figure 7.5 reveals only one prominent mode in the number of characters. Is the distribution unimodal, bimodal, or multimodal?30 Exercise 7.8 Height measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you anticipate in this height data set?31 7.1.4 Variance and standard deviation The mean is used to describe the center of a data set, but the variablity in the data is also important. Here, we introduce two measures of variability: the variance and the standard deviation. Both of these are very useful in data analysis, even though the formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to conceptually understand, and it roughly describes how far away the typical observation is from the mean. We call the distance of an observation from its mean its deviation. Below are the deviations for the \\(1^{st}_{}\\), \\(2^{nd}_{}\\), \\(3^{rd}\\), and \\(50^{th}_{}\\) observations in the num_char variable. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal. \\[\\begin{align*} x_1^{}-\\bar{x} &amp;= 21.7 - 11.6 = 10.1 \\hspace{5mm}\\text{ } \\\\ x_2^{}-\\bar{x} &amp;= 7.0 - 11.6 = -4.6 \\\\ x_3^{}-\\bar{x} &amp;= 0.6 - 11.6 = -11.0 \\\\ &amp;\\ \\vdots \\\\ x_{50}^{}-\\bar{x} &amp;= 15.8 - 11.6 = 4.2 \\end{align*}\\] If we square these deviations and then take an average, the result is about equal to the sample variance, denoted by \\(s_{}^2\\). \\[\\begin{align*} s_{}^2 &amp;= \\frac{10.1_{}^2 + (-4.6)_{}^2 + (-11.0)_{}^2 + \\cdots + 4.2_{}^2}{50-1} \\\\ &amp;= \\frac{102.01 + 21.16 + 121.00 + \\cdots + 17.64}{49} \\\\ &amp;= 172.44 \\end{align*}\\] We divide by \\(n-1\\), rather than dividing by \\(n\\), when computing the variance; you need not worry about this mathematical nuance for the material in this textbook. Notice that squaring the deviations does two things. First, it makes large values much larger, seen by comparing \\(10.1^2\\), \\((-4.6)^2\\), \\((-11.0)^2\\), and \\(4.2^2\\). Second, it gets rid of any negative signs. The standard deviation is the square root of the variance: \\[s=\\sqrt{172.44} = 13.13\\] The standard deviation of the number of characters in an email is about 13.13 thousand. A subscript of \\(_x\\) may be added to the variance and standard deviation, i.e. \\(s_x^2\\) and \\(s_x^{}\\), as a reminder that these are the variance and standard deviation of the observations represented by \\(x_1^{}\\), \\(x_2^{}\\), …, \\(x_n^{}\\). The \\(_{x}\\) subscript is usually omitted when it is clear which data the variance or standard deviation is referencing. Formulas and methods used to compute the variance and standard deviation for a population are similar to those used for a sample.32 However, like the mean, the population values have special symbols: \\(\\sigma_{}^2\\) for the variance and \\(\\sigma\\) for the standard deviation. The symbol \\(\\sigma\\) is the Greek letter sigma. Figure 7.7: In the num_char data, 41 of the 50 emails (82%) are within 1 standard deviation of the mean, and 47 of the 50 emails (94%) are within 2 standard deviations. Usually about 70% of the data are within 1 standard deviation of the mean and 95% are within 2 standard deviations, though this rule of thumb is less accurate for skewed data, as shown in this example. \\begin{tipBox}{ Focus on the conceptual meaning of the standard deviation as a descriptor of variability rather than the formulas. Usually 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, as seen in Figures and , these percentages are not strict rules.} \\end{tipBox} Figure 7.8: Three very different population distributions with the same mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). Exercise 7.9 Previously, the concept of shape of a distribution was introduced. A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using Figure 7.8 as an example, explain why such a description is important.33 Example 7.4 Describe the distribution of the num_char variable using the histogram in Figure 7.5. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context: the number of characters in emails. Also note any especially unusual cases. The distribution of email character counts is unimodal and very strongly skewed to the high end. Many of the counts fall near the mean at 11,600, and most fall within one standard deviation (13,130) of the mean. There is one exceptionally long email with about 65,000 characters. In practice, the variance and standard deviation are sometimes used as a means to an end, where the “end” is being able to accurately estimate the uncertainty associated with a sample statistic. For example, in Chapter ?? we will use the variance and standard deviation to assess how close the sample mean is to the population mean. 7.1.5 Box plots, quartiles, and the median A box plot summarizes a data set using five statistics while also plotting unusual observations. Figure 7.9 provides a vertical dot plot alongside a box plot of the num_char variable from the data set. Figure 7.9: A vertical dot plot next to a labeled box plot for the number of characters in 50 emails. The median (6,890), splits the data into the bottom 50% and the top 50%, marked in the dot plot by horizontal dashes and open circles, respectively. The first step in building a box plot is drawing a dark line denoting the median, which splits the data in half. Figure 7.9 shows 50% of the data falling below the median (dashes) and other 50% falling above the median (open circles). There are 50 character counts in the data set (an even number) so the data are perfectly split into two groups of 25. We take the median in this case to be the average of the two observations closest to the \\(50^{th}\\) percentile: \\((\\text{6,768} + \\text{7,012}) / 2 = \\text{6,890}\\). When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed). The second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The total length of the box, shown vertically in Figure 7.9, is called the interquartile range (IQR, for short). It, like the standard deviation, is a measure of variability in data. The more variable the data, the larger the standard deviation and IQR. The two boundaries of the box are called the first quartile (the \\(25^{th}\\) percentile, i.e. 25% of the data fall below this value) and the third quartile (the \\(75^{th}\\) percentile), and these are often labeled \\(Q_1\\) and \\(Q_3\\), respectively. Exercise 7.10 What percent of the data fall between \\(Q_1\\) and the median? What percent is between the median and \\(Q_3\\)?34 Extending out from the box, the whiskers attempt to capture the data outside of the box, however, their reach is never allowed to be more than \\(1.5\\times IQR\\).35 They capture everything within this reach. In Figure 7.9, the upper whisker does not extend to the last three points, which are beyond \\(Q_3 + 1.5\\times IQR\\), and so it extends only to the last point below this limit. The lower whisker stops at the lowest value, 33, since there is no additional data to reach; the lower whisker’s limit is not shown in the figure because the plot does not extend down to \\(Q_1 - 1.5\\times IQR\\). In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data. Any observation that lies beyond the whiskers is labeled with a dot. The purpose of labeling these points – instead of just extending the whiskers to the minimum and maximum observed values – is to help identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called outliers. In this case, it would be reasonable to classify the emails with character counts of 41,623, 42,793, and 64,401 as outliers since they are numerically distant from most of the data. Exercise 7.11 The observation 64,401, an outlier, was found to be an accurate observation. What would such an observation suggest about the nature of character counts in emails?36 Exercise 7.12 Using Figure 7.9, estimate the following values for num_char in the data set: (a) \\(Q_1\\), (b) \\(Q_3\\), and (c) IQR.37 7.1.6 Comparing numerical data across groups Some of the more interesting investigations can be considered by examining numerical data across groups. The methods required here aren’t really new. All that is required is to make a numerical plot for each group. Here two convenient methods are introduced: side-by-side box plots and hollow histograms. We will take a look again at the data set and compare the median household income for counties that gained population from 2000 to 2010 versus counties that had no gain. While we might like to make a causal connection here, remember that these are observational data and so such an interpretation would be unjustified. There were 2,041 counties where the population increased from 2000 to 2010, and there were 1,099 counties with no gain (all but one were a loss). A random sample of 100 counties from the first group and 50 from the second group are shown in Table 7.2 to give a better sense of some of the raw data. Table 7.2: In this table, median household income (in $1000s) from a random sample of 100 counties that gained population over 2000-2010 are shown on the left. Median incomes from a random sample of 50 counties that had no population gain are shown on the right. population gain no gain 41.2 33.1 30.4 37.3 79.1 34.5 40.3 33.5 34.8 22.9 39.9 31.4 45.1 50.6 59.4 29.5 31.8 41.3 47.9 36.4 42.2 43.2 31.8 36.9 28 39.1 42.8 50.1 27.3 37.5 53.5 26.1 57.2 38.1 39.5 22.3 57.4 42.6 40.6 48.8 28.1 29.4 43.3 37.5 47.1 43.8 26 33.8 35.7 38.5 42.3 43.7 36.7 36 41.3 40.5 68.3 31 46.7 30.5 35.8 38.7 39.8 68.3 48.3 38.7 62 37.6 32.2 46 42.3 48.2 42.6 53.6 50.7 35.1 30.6 56.8 38.6 31.9 31.1 66.4 41.4 34.3 38.9 37.3 41.7 37.6 29.3 30.1 51.9 83.3 46.3 48.4 40.8 42.6 57.5 32.6 31.1 44.5 34 48.7 45.2 34.7 32.2 46.2 26.5 40.1 39.4 38.6 40 57.3 45.2 33.1 38.4 46.7 25.9 43.8 71.7 45.1 32.2 63.3 54.7 36.4 41.5 45.7 71.3 36.3 36.4 41 37 66.7 39.7 37 37.7 50.2 45.8 45.7 60.2 53.1 21.4 29.3 50.1 35.8 40.4 51.5 66.4 36.1 43.6 39.8 The side-by-side box plot is a traditional tool for comparing across groups. An example is shown in the left panel of Figure 7.10, where there are two box plots, one for each group, placed into one plotting window and drawn on the same scale. Figure 7.10: Side-by-side box plot (left panel) and hollow histograms (right panel) for med_income, where the counties are split by whether there was a population gain or loss from 2000 to 2010. The income data were collected between 2006 and 2010. Another useful plotting method uses hollow histograms to compare numerical data across groups. These are just the outlines of histograms of each group put on the same plot, as shown in the right panel of Figure 7.10. Exercise 7.13 Use the plots in Figure 7.10 to compare the incomes for counties across the two groups. What do you notice about the approximate center of each group? What do you notice about the variability between groups? Is the shape relatively consistent between groups? How many prominent modes are there for each group?38 Exercise 7.14 What components of each plot in Figure 7.10 do you find most useful?39 7.1.7 Robust statistics How are the sample statistics of the num_char variable set affected by the observation, 64,401? What would have happened if this email wasn’t observed? What would happen to these summary statistics if the observation at 64,401 had been even larger, say 150,000? These scenarios are plotted alongside the original data in Figure 7.11, and sample statistics are computed under each scenario in Table 7.3. Figure 7.11: Dot plots of the original character count data and two modified data sets. Table: (#tab:robustOrNotTable) A comparison of how the median, IQR, mean (\\(\\bar{x}\\)), and standard deviation (\\(s\\)) change when extreme observations are present. | |————————–|:———-:|:——:|:————–:|:——:| | scenario | median | IQR | \\(\\bar{x}\\) | \\(s\\) | | original num_char data | 6,890 | 12,875 | 11,600 | 13,130 | | drop 66,924 observation | 6,768 | 11,702 | 10,521 | 10,798 | | move 66,924 to 150,000 | 6,890 | 12,875 | 13,310 | 22,434 | Exercise 7.15 (a) Which is more affected by extreme observations, the mean or median? Table 7.3 may be helpful. (b) Is the standard deviation or IQR more affected by extreme observations?40 The median and IQR are called robust estimates because extreme observations have little effect on their values. The mean and standard deviation are much more affected by changes in extreme observations. Example 7.5 The median and IQR do not change much under the three scenarios in Table 7.3. Why might this be the case? The median and IQR are only sensitive to numbers near \\(Q_1\\), the median, and \\(Q_3\\). Since values in these regions are relatively stable – there aren’t large jumps between observations – the median and IQR estimates are also quite stable. Exercise 7.16 The distribution of vehicle prices tends to be right skewed, with a few luxury and sports cars lingering out into the right tail. If you were searching for a new car and cared about price, should you be more interested in the mean or median price of vehicles sold, assuming you are in the market for a regular car?41 7.1.8 Transforming data (special topic) When data are very strongly skewed, we sometimes transform them so they are easier to model. Consider the histogram of salaries for Major League Baseball players’ salaries from 2010, which is shown in Figure 7.12(a). Figure 7.12: (a) Histogram of MLB player salaries for 2010, in millions of dollars. (b) Histogram of the log-transformed MLB player salaries for 2010. Example 7.6 The histogram of MLB player salaries is useful in that we can see the data are extremely skewed and centered (as gauged by the median) at about $1 million. What isn’t useful about this plot? Most of the data are collected into one bin in the histogram and the data are so strongly skewed that many details in the data are obscured. There are some standard transformations that are often applied when much of the data cluster near zero (relative to the larger values in the data set) and all observations are positive. A transformation is a rescaling of the data using a function. For instance, a plot of the natural logarithm42 of player salaries results in a new histogram in Figure 7.12(b). Transformed data are sometimes easier to work with when applying statistical models because the transformed data are much less skewed and outliers are usually less extreme. Transformations can also be applied to one or both variables in a scatterplot. A scatterplot of the line_breaks and num_char variables is shown in Figure 7.13(a), which was earlier shown in Figure 7.1. We can see a positive association between the variables and that many observations are clustered near zero. In Chapter ??, we might want to use a straight line to model the data. However, we’ll find that the data in their current state cannot be modeled very well. Figure 7.13(b) shows a scatterplot where both the line_breaks and num_char variables have been transformed using a log (base \\(e\\)) transformation. While there is a positive association in each plot, the transformed data show a steadier trend, which is easier to model than the untransformed data. Figure 7.13: (a) Scatterplot of line_breaks against num_char for 50 emails. (b) A scatterplot of the same data but where each variable has been log-transformed. Transformations other than the logarithm can be useful, too. For instance, the square root (\\(\\sqrt{\\text{original observation}}\\)) and inverse (\\(\\frac{1}{\\text{original observation}}\\)) are used by statisticians. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot. 7.1.9 Mapping data (special topic) The data set offers many numerical variables that we could plot using dot plots, scatterplots, or box plots, but these miss the true nature of the data. Rather, when we encounter geographic data, we should map it using an intensity map, where colors are used to show higher and lower values of a variable. Figures 7.14 and 7.15 shows intensity maps for federal spending per capita (fed_spend), poverty rate in percent (poverty), homeownership rate in percent (homeownership), and median household income (med_income). The color key indicates which colors correspond to which values. Note that the intensity maps are not generally very helpful for getting precise values in any given county, but they are very helpful for seeing geographic trends and generating interesting research questions. Figure 7.14: (a) Map of federal spending (dollars per capita). (b) Intensity map of poverty rate (percent). Figure 7.15: (a) Intensity map of homeownership rate (percent). (b) Intensity map of median household income ($1000s). Example 7.7 What interesting features are evident in the fed_spend and poverty intensity maps? The federal spending intensity map shows substantial spending in the Dakotas and along the central-to-western part of the Canadian border, which may be related to the oil boom in this region. There are several other patches of federal spending, such as a vertical strip in eastern Utah and Arizona and the area where Colorado, Nebraska, and Kansas meet. There are also seemingly random counties with very high federal spending relative to their neighbors. If we did not cap the federal spending range at $18 per capita, we would actually find that some counties have extremely high federal spending while there is almost no federal spending in the neighboring counties. These high-spending counties might contain military bases, companies with large government contracts, or other government facilities with many employees. Poverty rates are evidently higher in a few locations. Notably, the deep south shows higher poverty rates, as does the southwest border of Texas. The vertical strip of eastern Utah and Arizona, noted above for its higher federal spending, also appears to have higher rates of poverty (though generally little correspondence is seen between the two variables). High poverty rates are evident in the Mississippi flood plains a little north of New Orleans and also in a large section of Kentucky and West Virginia. Exercise 7.17 What interesting features are evident in the med_income intensity map?43 7.2 Categorical Data [MCR-TODO: MCR wrote these bullet points, are they addressed?] Conditional probability from contingency tables Bayes Theorem (law of total probability?) Like numerical data, categorical data can also be organized and analyzed. This section introduces tables and other basic tools for categorical data that are used throughout this book. The data set represents a sample from a larger email data set called . This larger data set contains information on 3,921 emails. In this section we will examine whether the presence of numbers, small or large, in an email provides any useful value in classifying email as spam or not spam. 7.2.1 Contingency tables and bar plots Table 7.4 summarizes two variables: spam and number. Recall that number is a categorical variable that describes whether an email contains no numbers, only small numbers (values under 1 million), or at least one big number (a value of 1 million or more). A table that summarizes data for two categorical variables in this way is called a contingency table. Each value in the table represents the number of times a particular combination of variable outcomes occurred. For example, the value 149 corresponds to the number of emails in the data set that are spam and had no number listed in the email. Row and column totals are also included. The row totals provide the total counts across each row (e.g. \\(149 + 168 + 50 = 367\\)), and column totals are total counts down each column. A table for a single variable is called a frequency table. Table 7.6 is a frequency table for the number variable. If we replaced the counts with percentages or proportions, the table would be called a relative frequency table. Table 7.4: A contingency table for spam and number. number none small big Total spam 149 168 50 367 spam not spam 400 2659 495 3554 Total 549 2827 545 3921 Table 7.5: A contingency table for spam and number. none small big Sum 1 149 168 50 367 0 400 2659 495 3554 Sum 549 2827 545 3921 Table 7.6: A frequency table for the number variable. none small big Sum 549 2827 545 3921 A bar plot is a common way to display a single categorical variable. The left panel of Figure 7.16 shows a bar plot for the number variable. In the right panel, the counts are converted into proportions (e.g. \\(549/3921=0.140\\) for none). Figure 7.16: Two bar plots of number. The left panel shows the counts, and the right panel shows the proportions in each group. 7.2.2 Row and column proportions Table 7.7 shows the row proportions for Table 7.4. The row proportions are computed as the counts divided by their row totals. The value 149 at the intersection of spam and none is replaced by \\(149/367=0.406\\), i.e. 149 divided by its row total, 367. So what does 0.406 represent? It corresponds to the proportion of spam emails in the sample that do not have any numbers. Table 7.7: A contingency table with row proportions for the spam and number variables. none small big Total spam \\(149/367 = 0.406\\) \\(168/367 = 0.458\\) \\(50/367 = 0.136\\) 1.000 not spam \\(400/3554 = 0.113\\) \\(2657/3554 = 0.748\\) \\(495/3554 = 0.139\\) 1.000 Total \\(549/3921 = 0.140\\) \\(2827/3921 = 0.721\\) \\(545/3921 = 0.139\\) 1.000 A contingency table of the column proportions is computed in a similar way, where each column proportion is computed as the count divided by the corresponding column total. Table 7.8 shows such a table, and here the value 0.271 indicates that 27.1% of emails with no numbers were spam. This rate of spam is much higher than emails with only small numbers (5.9%) or big numbers (9.2%). Because these spam rates vary between the three levels of number (none, small, big), this provides evidence that the spam and number variables are associated. Table 7.8: A contingency table with column proportions for the spam and number variables. none small big Total spam \\(149/549 = 0.271\\) \\(168/2827 = 0.059\\) \\(50/545 = 0.092\\) \\(367/3921 = 0.094\\) not spam \\(400/549 = 0.729\\) \\(2659/2827 = 0.941\\) \\(495/545 = 0.908\\) \\(3684/3921 = 0.906\\) Total 1.000 1.000 1.000 1.000 We could also have checked for an association between spam and number in Table 7.7 using row proportions. When comparing these row proportions, we would look down columns to see if the fraction of emails with no numbers, small numbers, and big numbers varied from spam to not spam. Exercise 7.18 What does 0.458 represent in Table 7.7? What does 0.059 represent in Table 7.8?44 Exercise 7.19 What does 0.139 at the intersection of not spam and big represent in Table 7.7? What does 0.908 represent in the Table 7.8?45 Example 7.8 Data scientists use statistics to filter spam from incoming email messages. By noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy. One of those characteristics is whether the email contains no numbers, small numbers, or big numbers. Another characteristic is whether or not an email has any HTML content. A contingency table for the spam and format variables from the data set are shown in Table 7.9. Recall that an HTML email is an email with the capacity for special formatting, e.g. bold text. In Table 7.9, which would be more helpful to someone hoping to classify email as spam or regular email: row or column proportions? Such a person would be interested in how the proportion of spam changes within each email format. This corresponds to column proportions: the proportion of spam in plain text emails and the proportion of spam in HTML emails. If we generate the column proportions, we can see that a higher fraction of plain text emails are spam (\\(209/1195 = 17.5\\%\\)) than compared to HTML emails (\\(158/2726 = 5.8\\%\\)). This information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam. Yet, when we carefully combine this information with many other characteristics, such as number and other variables, we stand a reasonable chance of being able to classify some email as spam or not spam. Table 7.9: A contingency table for spam and format. text HTML Total spam 209 158 367 not spam 986 2568 3554 Total 1195 2726 3921 Example 7.8 points out that row and column proportions are not equivalent. Before settling on one form for a table, it is important to consider each to ensure that the most useful table is constructed. Exercise 7.20 Look back to Tables 7.7 and 7.8. Which would be more useful to someone hoping to identify spam emails using the number variable?46 7.2.3 Segmented bar and mosaic plots Contingency tables using row or column proportions are especially useful for examining how two categorical variables are related. Segmented bar and mosaic plots provide a way to visualize the information in these tables. A segmented bar plot is a graphical display of contingency table information. For example, a segmented bar plot representing Table 7.8 is shown in Figure 7.17(a), where we have first created a bar plot using the number variable and then separated each group by the levels of spam. The column proportions of Table 7.8 have been translated into a standardized segmented bar plot in Figure 7.17(b), which is a helpful visualization of the fraction of spam emails in each level of number. Figure 7.17: (a) Segmented bar plot for numbers found in emails, where the counts have been further broken down by spam. (b) Standardized version of Figure (a). Example 7.9 Examine both of the segmented bar plots. Which is more useful? Figure 7.17(a) contains more information, but Figure 7.17(b) presents the information more clearly. This second plot makes it clear that emails with no number have a relatively high rate of spam email – about 27%! On the other hand, less than 10% of email with small or big numbers are spam. Since the proportion of spam changes across the groups in Figure 7.17(a), we can conclude the variables are dependent, which is something we were also able to discern using table proportions. Because both the none and big groups have relatively few observations compared to the small group, the association is more difficult to see in Figure 7.17(b). In some other cases, a segmented bar plot that is not standardized will be more useful in communicating important information. Before settling on a particular segmented bar plot, create standardized and non-standardized forms and decide which is more effective at communicating features of the data. Figure 7.18: (a) The one-variable mosaic plot for number and (b) the two-variable mosaic plot for both number and spam. A mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. Figure 7.18(a) shows a mosaic plot for the number variable. Each column represents a level of number, and the column widths correspond to the proportion of emails of each number type. For instance, there are fewer emails with no numbers than emails with only small numbers, so the no number email column is slimmer. In general, mosaic plots use box areas to represent the number of observations. Figure 7.19: Mosaic plot where emails are grouped by the number variable after they have been divided into spam and not spam. This one-variable mosaic plot is further divided into pieces in Figure 7.18(b) using the spam variable. Each column is split proportionally according to the fraction of emails that were spam in each number category. For example, the second column, representing emails with only small numbers, was divided into emails that were spam (lower) and not spam (upper). As another example, the bottom of the third column represents spam emails that had big numbers, and the upper part of the third column represents regular emails that had big numbers. We can again use this plot to see that the spam and number variables are associated since some columns are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized version of the segmented bar plot. In a similar way, a mosaic plot representing row proportions of Table 7.4 could be constructed, as shown in Figure 7.19. However, because it is more insightful for this application to consider the fraction of spam in each category of the number variable, we prefer Figure 7.18(b). 7.2.4 The only pie chart you will see in this book While pie charts are well known, they are not typically as useful as other charts in a data analysis. A pie chart is shown in Figure 7.20 alongside a bar plot. It is generally more difficult to compare group sizes in a pie chart than in a bar plot, especially when categories have nearly identical counts or proportions. In the case of the none and big categories, the difference is so slight you may be unable to distinguish any difference in group sizes for either plot! Figure 7.20: A pie chart and bar plot of number for the email data set. Answers may vary. Scatterplots are helpful in quickly spotting associations between variables, whether those associations represent simple or more complex relationships.↩ Subset of data from http://www.amstat.org/publications/jse/v1n1/datasets.lock.html↩ Consider the case where your vertical axis represents something “good” and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description since water becomes toxic when consumed in excessive quantities.↩ \\(x_1\\) corresponds to the number of characters in the first email in the sample (21.7, in thousands), \\(x_2\\) to the number of characters in the second email (7.0, in thousands), and \\(x_i\\) corresponds to the number of characters in the \\(i^{th}\\) email in the data set.↩ The sample size was \\(n=50\\).↩ Other ways to describe data that are skewed to the right: skewed to the right, skewed to the high end, or skewed to the positive end.↩ The skew is visible in all three plots, though the flat dot plot is the least useful. The stacked dot plot and histogram are helpful visualizations for identifying skew.↩ Character counts for individual emails.↩ Another definition of mode, which is not typically used in statistics, is the value with the most occurrences. It is common to have no observations with the same value in a data set, which makes this other definition useless for many real data sets.↩ Unimodal. Remember that uni stands for 1 (think unicycles). Similarly, bi stands for 2 (think bicycles). (We’re hoping a multicycle will be invented to complete this analogy.)↩ There might be two height groups visible in the data set: one of the students and one of the adults. That is, the data are probably bimodal.↩ The only difference is that the population variance has a division by \\(n\\) instead of \\(n-1\\).↩ Figure 7.8 shows three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution.↩ Since \\(Q_1\\) and \\(Q_3\\) capture the middle 50% of the data and the median splits the data in the middle, 25% of the data fall between \\(Q_1\\) and the median, and another 25% falls between the median and \\(Q_3\\).↩ While the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots.↩ That occasionally there may be very long emails.↩ These visual estimates will vary a little from one person to the next: \\(Q_1\\approx\\) 3,000, \\(Q_3\\approx\\) 15,000, $=Q_3 - Q_1 $ 12,000. (The true values: \\(Q_1=\\) 2,536, \\(Q_3=\\) 15,411, $ = $ 12,875.)↩ Answers may vary a little. The counties with population gains tend to have higher income (median of about $45,000) versus counties without a gain (median of about $40,000). The variability is also slightly larger for the population gain group. This is evident in the IQR, which is about 50% bigger in the gain group. Both distributions show slight to moderate right skew and are unimodal. There is a secondary small bump at about $60,000 for the no gain group, visible in the hollow histogram plot, that seems out of place. (Looking into the data set, we would find that 8 of these 15 counties are in Alaska and Texas.) The box plots indicate there are many observations far above the median in each group, though we should anticipate that many observations will fall beyond the whiskers when using such a large data set.↩ Answers will vary. The side-by-side box plots are especially useful for comparing centers and spreads, while the hollow histograms are more useful for seeing distribution shape, skew, and groups of anomalies.↩ (a) Mean is affected more. (b) Standard deviation is affected more. Complete explanations are provided in the material following Guided Practice 7.15.↩ Buyers of a “regular car” should be concerned about the median price. High-end car sales can drastically inflate the mean price while the median will be more robust to the influence of those sales.↩ Statisticians often write the natural logarithm as \\(\\log\\). You might be more familiar with it being written as \\(\\ln\\).↩ Note: answers will vary. There is a very strong correspondence between high earning and metropolitan areas. You might look for large cities you are familiar with and try to spot them on the map as dark spots.↩ 0.458 represents the proportion of spam emails that had a small number. 0.058 represents the fraction of emails with small numbers that are spam.↩ 0.139 represents the fraction of non-spam email that had a big number. 0.908 represents the fraction of emails with big numbers that are non-spam emails.↩ The column proportions in Table 7.8 will probably be most useful, which makes it easier to see that emails with small numbers are spam about 5.9% of the time (relatively rare). We would also see that about 27.1% of emails with no numbers are spam, and 9.2% of emails with big numbers are spam.↩ "],
["cor-reg.html", "Chapter 8 Correlation and Regression 8.1 Visual summaries of data: scatterplot, side-by-side boxplots, histogram, density plot, box plot (lead out with multivariate, follow with univariate) 8.2 Describing distributions: correlation, central tendency, variability, skew, modality 8.3 Num vs. num - SLR", " Chapter 8 Correlation and Regression 8.1 Visual summaries of data: scatterplot, side-by-side boxplots, histogram, density plot, box plot (lead out with multivariate, follow with univariate) 8.2 Describing distributions: correlation, central tendency, variability, skew, modality 8.3 Num vs. num - SLR correlation Line fitting, residuals, and correlation Fitting a line by least squares regression Types of outliers in linear regression "],
["mult-reg.html", "Chapter 9 Multiple Regression 9.1 Num vs. whatever - MLR 9.2 Parallel slopes 9.3 Hint at interaction, planes, and parallel planes but not quantify 9.4 Logistic regression", " Chapter 9 Multiple Regression 9.1 Num vs. whatever - MLR Introduction to multiple regression 9.2 Parallel slopes 9.3 Hint at interaction, planes, and parallel planes but not quantify Visualization of higher-dimensional models (rgl demo) 9.4 Logistic regression Binary vs. num/whatever Three scales interpretation (e.g. probability, odds, log-odds) “parallel” logistic curves? "],
["inference-foundations.html", "Chapter 10 Foundations of inference 10.1 Understanding inference through simulation 10.2 Randomization case study: gender discrimination 10.3 Randomization case study: opportunity cost 10.4 Hypothesis testing 10.5 Confidence intervals 10.6 Simulation case studies", " Chapter 10 Foundations of inference 10.1 Understanding inference through simulation 10.2 Randomization case study: gender discrimination 10.3 Randomization case study: opportunity cost 10.4 Hypothesis testing 10.5 Confidence intervals 10.6 Simulation case studies "],
["inference-cat.html", "Chapter 11 Inference for categorical data 11.1 Inference for a single proportion 11.2 Difference of two proportions 11.3 Testing for goodness of fit using chi-square (special topic, include simulation version) 11.4 Testing for independence in two-way tables (special topic)", " Chapter 11 Inference for categorical data 11.1 Inference for a single proportion Simulation Exact (if we include course on probability) CLT and Normal approximation 11.2 Difference of two proportions 11.3 Testing for goodness of fit using chi-square (special topic, include simulation version) 11.4 Testing for independence in two-way tables (special topic) "],
["inference-num.html", "Chapter 12 Inference for numerical data 12.1 One-sample means 12.2 Paired data 12.3 Difference of two means 12.4 Comparing many means with ANOVA (special topic, include simulation version)", " Chapter 12 Inference for numerical data 12.1 One-sample means Bootstrap (for means, medians) t-distribution 12.2 Paired data 12.3 Difference of two means 12.4 Comparing many means with ANOVA (special topic, include simulation version) "],
["inference-reg.html", "Chapter 13 Inference for regression 13.1 Inference for linear regression 13.2 Checking model assumptions using graphs 13.3 Inference for multiple regression 13.4 Inference for logistic regression", " Chapter 13 Inference for regression 13.1 Inference for linear regression Bootstrap for regression coefficients t-distribution for regression coefficients Model Comparison: Occam’s Razor and R^2 &gt; R^2_adj 13.2 Checking model assumptions using graphs L-I-N-E 13.3 Inference for multiple regression residuals vs. fitted instead of residuals vs. x 13.4 Inference for logistic regression "],
["probability.html", "Chapter 14 Appendix: Probability", " Chapter 14 Appendix: Probability (Keep same content as before, minus the bit of probability that got moved to categorical EDA) "],
["references.html", "References", " References "]
]
