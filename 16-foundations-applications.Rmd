# Applications: Foundations {#foundations-applications}

## Foundations Summary {#foundations-sec-summary}

In the Foundations of inference chapters, we have provided three different methods for statistical inference.
We will continue to build on all three of the methods throughout the text, and by the end, you should have an understanding of their similarities and differences between them.
Meanwhile, it is important to note that the methods are designed to mimic variability with data, and we know that variability can come from different sources (e.g., random sampling vs. random allocation, see Figure \@ref(fig:randsampValloc)).
In Table \@ref(tab:foundations-summary), we have summarized some of the ways the inferential procedures feature specific sources of variability.
We hope that you refer back to the table often as you dive more deeply into inferential ideas in future chapters.

```{r foundations-summary}
inference_method_summary_table %>%
  filter(question != "What are the technical conditions?") %>%
  kable(
    caption = "Summary and comparison of randomization, bootstrapping, and mathematical models as inferential statistical methods.", 
    col.names = c("", "Randomization", "Bootstrapping", "Mathematical models")
    ) %>%
  kable_styling()
```

## Case study: Malaria vaccine {#case-study-malaria-vaccine}

In this case study, we consider a new malaria vaccine called PfSPZ.
In the malaria study, volunteer patients were randomized into one of two experiment groups: 14 patients received an experimental vaccine and 6 patients received a placebo vaccine.
Nineteen weeks later, all 20 patients were exposed to a drug-sensitive malaria virus strain; the motivation of using a drug-sensitive strain of virus here is for ethical considerations, allowing any infections to be treated effectively.

::: {.data data-latex=""}
The data from this study can be found in the [openintro](http://openintrostat.github.io/openintro) package: [`malaria`](http://openintrostat.github.io/openintro/reference/malaria.html).
:::

The results are summarized in Table \@ref(tab:malaria-vaccine-20-ex-summary), where 9 of the 14 treatment patients remained free of signs of infection while all of the 6 patients in the control group patients showed some baseline signs of infection.

```{r malaria-vaccine-20-ex-summary}
malaria %>%
  count(treatment, outcome, .drop = FALSE) %>%
  pivot_wider(names_from = outcome, values_from = n) %>%
  adorn_totals(where = c("row", "col")) %>%
  kable(caption = "Summary results for the malaria vaccine experiment.")
```

::: {.guidedpractice data-latex=""}
Is this an observational study or an experiment?
What implications does the study type have on what can be inferred from the results?[^foundations-applications-1]
:::

[^foundations-applications-1]: The study is an experiment, as patients were randomly assigned an experiment group.
    Since this is an experiment, the results can be used to evaluate a causal relationship between the malaria vaccine and whether patients showed signs of an infection.

### Variability within data

In this study, a smaller proportion of patients who received the vaccine showed signs of an infection (35.7% versus 100%).
However, the sample is very small, and it is unclear whether the difference provides *convincing evidence* that the vaccine is effective.

::: {.workedexample data-latex=""}}
Statisticians and data scientists are sometimes called upon to evaluate the strength of evidence.
When looking at the rates of infection for patients in the two groups in this study, what comes to mind as we try to determine whether the data show convincing evidence of a real difference?

------------------------------------------------------------------------

The observed infection rates (35.7% for the treatment group versus 100% for the control group) suggest the vaccine may be effective.
However, we cannot be sure if the observed difference represents the vaccine's efficacy or is just from random chance.
Generally there is a little bit of fluctuation in sample data, and we wouldn't expect the sample proportions to be *exactly* equal, even if the truth was that the infection rates were independent of getting the vaccine.
Additionally, with such small samples, perhaps it's common to observe such large differences when we randomly split a group due to chance alone!
:::

This example is a reminder that the observed outcomes in the data sample may not perfectly reflect the true relationships between variables since there is **random noise**.
While the observed difference in rates of infection is large, the sample size for the study is small, making it unclear if this observed difference represents efficacy of the vaccine or whether it is simply due to chance.
We label these two competing claims, $H_0$ and $H_A,$ which are spoken as "H-nought" and "H-A":

-   $H_0$: **Independence model.** The variables and are independent.
    They have no relationship, and the observed difference between the proportion of patients who developed an infection in the two groups, 64.3%, was due to chance.

-   $H_A$: **Alternative model.** The variables are *not* independent.
    The difference in infection rates of 64.3% was not due to chance.
    Here (because an experiment was done), if the difference in infection rate is not due to change, it was the vaccine that affected the rate of infection.

What would it mean if the independence model, which says the vaccine had no influence on the rate of infection, is true?
It would mean 11 patients were going to develop an infection *no matter which group they were randomized into*, and 9 patients would not develop an infection *no matter which group they were randomized into*.
That is, if the vaccine did not affect the rate of infection, the difference in the infection rates was due to chance alone in how the patients were randomized.

Now consider the alternative model: infection rates were influenced by whether a patient received the vaccine or not.
If this was true, and especially if this influence was substantial, we would expect to see some difference in the infection rates of patients in the groups.

We choose between these two competing claims by assessing if the data conflict so much with $H_0$ that the independence model cannot be deemed reasonable.
If this is the case, and the data support $H_A,$ then we will reject the notion of independence and conclude the vaccine was effective.

### Simulating the study

We're going to implement **simulation** under the setting where we will pretend we know that the malaria vaccine being tested does *not* work.
Ultimately, we want to understand if the large difference we observed in the data is common in these simulations that represent independence.
If it is common, then maybe the difference we observed was purely due to chance.
If it is very uncommon, then the possibility that the vaccine was helpful seems more plausible.

Table \@ref(tab:malaria-vaccine-20-ex-summary) shows that 11 patients developed infections and 9 did not.
For our simulation, we will suppose the infections were independent of the vaccine and we were able to *rewind* back to when the researchers randomized the patients in the study.
If we happened to randomize the patients differently, we may get a different result in this hypothetical world where the vaccine doesn't influence the infection.
Let's complete another **randomization** using a simulation.

In this **simulation**, we take 20 notecards to represent the 20 patients, where we write down "infection" on 11 cards and "no infection" on 9 cards.
In this hypothetical world, we believe each patient that got an infection was going to get it regardless of which group they were in, so let's see what happens if we randomly assign the patients to the treatment and control groups again.
We thoroughly shuffle the notecards and deal 14 into a pile and 6 into a pile.
Finally, we tabulate the results, which are shown in Table \@ref(tab:malaria-vaccine-20-ex-summary-rand-1).

```{r malaria-vaccine-20-ex-summary-rand-1}
# matching OS4
malaria_rand <- tibble(
  treatment = c(rep("infection", 11), 
                rep("no infection", 9)),
  outcome   = c(rep("vaccine", 7), rep("placebo", 4),
                rep("vaccine", 7), rep("placebo", 2))
)

malaria_rand %>%
  count(treatment, outcome, .drop = FALSE) %>%
  pivot_wider(names_from = outcome, values_from = n) %>%
  adorn_totals(where = c("row", "col")) %>%
  kable(caption = "Simulation results, where any difference in infection ratio is purely due to chance.")
```

::: {.guidedpractice data-latex=""}
How does this compare to the observed 64.3% difference in the actual data?[^foundations-applications-2]
:::

[^foundations-applications-2]: $4 / 6 - 7 / 14 = 0.167$ or about 16.7% in favor of the vaccine.
    This difference due to chance is much smaller than the difference observed in the actual groups.

### Checking for independence

We computed one possible difference under the independence model in the previous Guided Practice, which represents one difference due to chance.
While in this first simulation, we physically dealt out notecards to represent the patients, it is more efficient to perform the simulation using a computer.

Repeating the simulation on a computer, we get another difference due to chance: $$ \frac{2}{6{}} - \frac{9}{14{}} = -0.310 $$

And another: $$ \frac{3}{6{}} - \frac{8}{14{}} = -0.071$$

And so on until we repeat the simulation enough times to create a *distribution of differences from chance alone*.

Figure \@ref(fig:malaria-rand-dot-plot) shows a stacked plot of the differences found from 100 simulations, where each dot represents a simulated difference between the infection rates (control rate minus treatment rate).

```{r malaria-rand-dot-plot, fig.cap = "A stacked dot plot of differences from 100 simulations produced under the independence mode, $H_0,$ where in these simulations infections are unaffected by the vaccine. Two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study."}
set.seed(19)
malaria %>%
  specify(response = outcome, explanatory = treatment, success = "infection") %>%
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "diff in props", order = c("placebo", "vaccine")) %>%
  # simplify by rounding
  mutate(stat = round(stat, 3)) %>%
  ggplot(aes(x = stat)) +
  geom_dotplot(binwidth = 0.1, dotsize = 0.2) +
  labs(y = NULL, x = "Difference in infection rates") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  gghighlight(stat >= 0.643)
```

Note that the distribution of these simulated differences is centered around 0.
We simulated these differences assuming that the independence model was true, and under this condition, we expect the difference to be near zero with some random fluctuation, where *near* is pretty generous in this case since the sample sizes are so small in this study.

::: {.workedexample data-latex=""}}
How often would you observe a difference of at least 64.3% (0.643) according to Figure \@ref(fig:malaria-rand-dot-plot)?
Often, sometimes, rarely, or never?

------------------------------------------------------------------------

It appears that a difference of at least 64.3% due to chance alone would only happen about 2% of the time according to Figure \@ref(fig:malaria-rand-dot-plot).
Such a low probability indicates a rare event.
:::

The difference of 64.3% being a rare event suggests two possible interpretations of the results of the study:

-   $H_0$: **Independence model.** The vaccine has no effect on infection rate, and we just happened to observe a difference that would only occur on a rare occasion.

-   $H_A$: **Alternative model.** The vaccine has an effect on infection rate, and the difference we observed was actually due to the vaccine being effective at combating malaria, which explains the large difference of 64.3%.

Based on the simulations, we have two options.
(1) We conclude that the study results do not provide strong evidence against the independence model.
That is, we do not have sufficiently strong evidence to conclude the vaccine had an effect in this clinical setting.
(2) We conclude the evidence is sufficiently strong to reject $H_0$ and assert that the vaccine was useful.
When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event.[\^This reasoning does not generally extend to anecdotal observations.
Each of us observes incredibly rare events every day, events we could not possibly hope to predict.
However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous.
For example, we might look at the lottery: there was only a 1 in 292 million chance that the Powerball numbers for the largest jackpot in history (January 13th, 2016) would be (04, 08, 19, 27, 34) with a Powerball of (10), but nonetheless those numbers came up!
However, no matter what numbers had turned up, they would have had the same incredibly rare odds.
That is, *any set of numbers we could have observed would ultimately be incredibly rare*.
This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare.
We should be cautious not to misinterpret such anecdotal evidence.] So in the vaccine case, we reject the independence model in favor of the alternative.
That is, we are concluding the data provide strong evidence that the vaccine provides some protection against malaria in this clinical setting.

One field of statistics, statistical inference, is built on evaluating whether such differences are due to chance.
In statistical inference, data scientists evaluate which model is most reasonable given the data.
Errors do occur, just like rare events, and we might choose the wrong model.
While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often decision errors occur.

## Interactive R tutorials {#foundations-tutorials}

Navigate the concepts you've learned in this chapter in R using the following self-paced tutorials.
All you need is your browser to get started!

::: {.alltutorials}
[Tutorial 5: Introduction to statistical inference](https://openintrostat.github.io/ims-tutorials/05-introduction-to-statistical-inference/)
:::

::: {.singletutorial}
[Tutorial 5 - Lesson 1: Sampling variability](https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-01/)
:::

::: {.singletutorial}
[Tutorial 5 - Lesson 2: Randomization test](https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-02/)
:::

::: {.singletutorial}
[Tutorial 5 - Lesson 3: Errors in hypothesis testing](https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-03/)
:::

::: {.singletutorial}
[Tutorial 5 - Lesson 4: Parameters and confidence intervals](https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-04/)
:::

You can also access the full list of tutorials supporting this book [here](https://openintrostat.github.io/ims-tutorials/).

## R labs {#foundations-labs}

Further apply the concepts you've learned in this chapter in R with computational labs that walk you through a data analysis case study.

::: {.singlelab}
[Sampling distributions - Does science benefit you?](https://openintro.shinyapps.io/sampling_distributions/)
:::

::: {.singlelab}
[Confidence intervals - Climate change](https://openintro.shinyapps.io/confidence_intervals/)
:::

::: {.alllabs}
[Full list of labs supporting OpenIntro::Introduction to Modern Statistics](http://openintrostat.github.io/oilabs-tidy/)
:::
